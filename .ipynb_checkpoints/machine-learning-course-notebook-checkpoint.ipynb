{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: Introduction\n",
    "\n",
    "> **Machine Learning**: getting computers to learn how to do something, rather than being explicitly programmed how to do it.\n",
    "\n",
    "ML is hot right now b/c of the explosion of large data sets e.g. web, genome, electronic medical records. ML applications includes natural language processing and image processing, and customization ala Netflix or Amazon.\n",
    "\n",
    "### Supervised Learning\n",
    "> **Supervised Learning**: algorithm where computer is given a data set which contains \"right answers\" for the thing we want to predict.\n",
    "\n",
    "- **Regression Problem**: predict a continuous-valued output, e.g. predict house price from square footage by fitting a line or quadratic.\n",
    "- **Classification Problem**: predict discrete-valued output e.g. predict benign vs. malignant from tumor size and age by fitting a line that separates the two groups in the parameter space plane.\n",
    "\n",
    "Some algorithms may actually use an infinite number of features to make their predictions/classifications (SVMs).\n",
    "\n",
    "### Unsupervised Learning\n",
    "> **Unsupervised Learning**: data set does not contain \"right answers\", rather we are just asked to find some structure within it.\n",
    "\n",
    "- **Clustering**: a type of unsupervised ML where we try to group the data points into clusters.\n",
    "An example is news.google.com which clusters articles on the web into groups based on what event/topic they are about. Another example is a data set of people for which each person has a set of values indicating presence of various genes, the algorithm would cluster the people into groups. More:\n",
    "    - grouping customers into market segments\n",
    "    - grouping persons into \"friend\" clusters from a social network\n",
    "    - grouping computers based on how much they interact in a cluster\n",
    "\n",
    "- **Cocktail Party**: a type of unsupervised learning which disentangles data to give the inputs that created it. For example, two people are talking at the same time in a cocktail party which has microphones stationed at various spots. From multiple microphone recordings, identify what the two speakers were individually saying. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span> <span style=\"background-color:#FFFFbb\">ML algorithms are ways for a computer to learn how to do something without that \"how\" being explicitly programmed. Supervised algorithms take in \"labeled\" data and then give you a function to predict the label for new data points (labels can be continuous as in regression or discrete as in classification). Unsupervised algorithms find different kinds of structure in \"unlabeled\" data, such as whether the points fall into well-defined clusters, or whether the data can be decomposed into disparate sources (\"Cocktail Party\").</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: Linear Regression with One Variable\n",
    "\n",
    "## Model and Cost Function\n",
    "The **features** are the input variables, $x$, (like house square footage). The output variables are those which we want to predict, $y$. A supervised learning algorithm takes as input a **training set** which is a data set of points $(x, y)$ and it outputs a function $y=h(x)$ called the **Hypothesis**, that can be used to predict the output value for new data. \n",
    "\n",
    "> **Univariate Linear Regression**: a very common and simple algorithm in which the hypothesis is of the form $h_\\theta(x) = \\theta_0 + \\theta_1x$.\n",
    "\n",
    "We need our algorithm to select a specific $h$ i.e. specific values of the parameters based on the training set. **Key idea is to choose values of parameters so that $h(x_i))$ is *close* to $y_i$ for all data points in the training set.** An example of formalizing this is to minimize $\\Sigma_i (h(x_i)-y_i)^2$ over the space of $(\\theta_0, \\theta_1)$. \n",
    "\n",
    "A **cost function** is the general formalization of this - and our task is to choose the specific value of $\\vec{\\theta}$ from which minimizes some reasonable cost function $J(\\vec{\\theta}; \\{(x_i, y_i)\\})$. The specific cost function mentioned above is called **squared error cost function** and it works very well for most regression problems:\n",
    "\n",
    "\\begin{align}\n",
    "J_{LSQ}(\\theta) = \\frac{1}{m}\\sum_1^m\\frac{1}{2}(h(x_i)-y_i)^2\n",
    "\\end{align}\n",
    "\n",
    "For some fixed training set, some form of hypothesis, and a one or two-dimensional parameter space you can imagine calculating and plotting the value of $J$ at every point in the parameter space - hopefully it has a bowl like shape with some nice minimum. \n",
    "\n",
    "<img src=\"images/pic2.png\" width=550></img>\n",
    "\n",
    "**In linear regression we want an algorithm for selecting the value of the parameters, $\\vec{\\theta}$ that give us a hypothesis, $h(x)$, which minimizes the cost function, $J$, over the space of parameter values.** One option for such an algorithm is discussed below.\n",
    "\n",
    "## Parameter Learning\n",
    "**Gradient Descent** is an algorithm for minimizing $J$ over $\\theta$ space in linear regression problems as well as lots of other problems. The basic idea is to start at some specific value, move away from that value in the direction that decrease $J$ as much as possible, and repeat until we reach a point where no direction can decrease $J$ i.e. a minimum! Which local minimum you reach is very sensitive to starting point in this algorithm. \n",
    "\n",
    "Sidenote, in theoretical computer science we use $:=$ to denote variable assignment while $=$ denotes assertion of equality. With that in mind, we can formally write:\n",
    "> **Gradient Descent**: Repeat $\\theta_j := \\theta_j - \\alpha\\frac{\\partial}{\\partial \\theta_j}J$ until convergence\n",
    "\n",
    "Picture each $\\theta_j$ as representing a unique independent direction in parameter space. If we are only allowed to move a fixed distance from our starting point then we want to allocate this movement (in the sense of vector components) amongst these independent directions according to which directions can give large $\\Delta J$ from that particular point. This is why the larger the derivative in a particular direction, the more you choose to change the value i.e. move away in that direction. Regarding the sign, a positive derivative means a positive change in $\\theta_j$ gives a positive change in $J$, which is why you would then *subtract* an amount in order to get a *negative* change in J because we want to minimize it.\n",
    "\n",
    "<img src=\"images/pic3.png\" width=550></img>\n",
    "\n",
    "In the above $\\alpha$ captures how large of a step we are taking each time, and is called the **learning rate**. If it is too big then it can actually cause divergence (imagine ping ponging back and forth over the minimum, overshooting by more each time b/c the slope (derivative is getting steeper). You might imagine that for any non-zero alpha you could never really get exactly to the minimum point, but because the derivative approaches zero at a minimum you actually can end up moving in smaller and smaller steps to the true minimum, as opposed to just ping ponging back and forth over it. \n",
    "\n",
    "**Our first complete ML algorithm is built by using gradient descent method to minimize the specific least squares cost function for Univariate Linear Regression**. For squared error cost functions, these problems always give us a plot of $J$ which is bowl-shaped (*convex*) having one minimum. When you plug in the specific squared error cost function then you can do the partial derivatives in the gradient descent recipe and you find:\n",
    "\n",
    "> **Gradient Descent with Squared Error**: $ \\theta_j := \\theta_j - \\alpha\\frac{1}{m}\\sum^m_{i=1}\\big(h(x^{(i)})-y^{(i)}\\big)x^{(i)}$\n",
    "\n",
    "A couple details: Batch Gradient Descent means at each point when we compute the partial derivatives of $J$ we are using all the data points we have to calculate those values. For squared error cost linear regression there is an analytical solution for the minimum, this approach is called the Normal Equation Method. Surprisingly though gradient descent actually scales better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span> <span style=\"background-color:#FFFFbb\">A supervised learning algorithm takes a \"training\" data set, $(x, y)$, and outputs a hypothesis, $y=h_\\vec{\\theta}(x)$, that can be used to predict the $y$-value for new data. In regression problems, the algorithm chooses the particular $\\vec{\\theta}$ value that minimizes a cost function, $J$. The cost function quantifies the distance from the hypothesis predictions to the actual values in the training data set. A common cost function is squared error, and a common hypothesis form is $h(x)=\\theta_0+\\theta_1x$ - together this defines a Univarite Linear Regression Problem. Gradient descent is an implementation of how to find the $\\vec{\\theta}$ value - it works by starting at some point in parameter space and moving a short distance in the direction which gives the largest decrease in $J$, and repeating that until converence. Explicitly: $\\theta_j := \\theta_j - \\alpha\\frac{\\partial}{\\partial \\theta_j}J$</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Week 2: Linear Regression With Multiple Variables\n",
    "\n",
    "Now we have multiple features that we want to use to predict the y-values of new data points, and we still want to maintain a simple linear form for our hypothesis. Some conventions: $x^{(i)}$ is a vector holding the feature values for the $i^{th}$ data point in our training set and we use $x$ to refer generally to such a vector of $n$ features with the one complication that we define $x^{(i)}_0 = 1$ for all $i$. This allows us to write our multivariate linear hypothesis in a convenient linear algebra form: $h_\\theta(x) = \\theta^Tx$ where $\\theta$ is a vector of the parameters $\\{\\theta_0,...\\theta_n\\}$ and $x$ is as described above, $\\{1, x_1,...x_n\\}$.\n",
    "\n",
    "To implement this multivariate linear regression with squared error loss you can use the update rule written in the above section. Below are some tricks for actually running this algorithm.\n",
    "\n",
    "### Trick for Gradient Descent: Feature Scaling\n",
    "Imagine two features, one with range 0 to 2000 and one with range 0 to 5. We probably expect the data to be sensitive to both features in the sense of proportional change i.e. going from 1000 to 2000 in the first feature may give a similar change as going from 2 to 4 in the second. In this case if you imagine a contour plot of the cost function the concentric contours will be extremely elongated ellipses and so gradient descent with a reasonably large alpha may wind up with extremely different gradient directions from one spot to the next, because the normal to the contour changes so sharply near the endpoints of the ellipse. If instead we scale both features to their range, the contour plot becomes basically circles, and the gradient descent will be more direct. It is also common to normalize features to their mean value. So in general:\n",
    "\n",
    "> **Feature Scaling and Normalization**: $x_j \\rightarrow \\frac{x_j - \\textrm{average}(x_j)}{\\textrm{range}(x_j)}$\n",
    "\n",
    "<img src=\"images/pic4.png\" width=550></img>\n",
    "\n",
    "### Trick for Gradient Descent: Learning Rate\n",
    "For a given learning rate you can **plot $J$ as a function of the cycle number i.e. number of iterations when the algorithm is running.** You should see $J$ decrease to a mostly flat value (converge) over some reasonable number of iterations. If $J$ increases, or is periodically decreasing and then increasing again you should try a smaller learning rate $\\alpha$. A good trick is to try your algorithm for a set of values like $\\alpha = \\{0.001, 0.003, 0.01, 0.03....1, 3\\}$ and inspect the $J$ convergence plots to identify the best rate.\n",
    "\n",
    "<img src=\"images/pic5.png\" width=550></img>\n",
    "\n",
    "When choosing the form of your hypothesis you are free to combine and transform your existing features in any way you like to achieve new features. For example, if you have frontage and depth data for a set of housing lots, you might decide that `area = frontage * depth` is actually the relevant quantity for your model.\n",
    "\n",
    "> **Polynomial Regression:** Linear regression where you define new feaures that are powers of the original features so that $h_\\theta = \\theta_0 + \\theta_1x + \\theta_2x^2 + \\theta_3x^3....$. Note that feature scaling can become very important here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span> <span style=\"background-color:#FFFFbb\">Multivariate linear regression predicts labels based on multiple input features, $\\{x_j\\}$, so that $h_\\theta(x) = (\\theta_0,...\\theta_n)^T\\cdot(1, x_1,...x_n)$. You can transform and combine features to get new features for your model such as $x_1x_2$ or $\\sqrt{x_3}$, and  polynomial regression is a common case of this where you build powers of a feature. Gradient descent for multivariate linear regression works much better if you use a normalized and scaled version of your features, $x_j \\rightarrow \\frac{x_j - \\textrm{average}(x_j)}{\\textrm{range}(x_j)}$, and plotting $J$ during your algorithm can help you find an optimal \"step size\" coefficient $\\alpha$.</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2: Computing Parameters Analytically\n",
    "\n",
    ">**Normal Equation**: an alternative to gradient descent with squared error loss that solves for the minimal $\\theta$ analytically.\n",
    "\n",
    "When you have written out the specific form of cost function $J$, such as for squared error loss, we know that the optima for $J$ in $\\theta$ space are points that satisfy $\\frac{\\partial}{\\partial\\theta_j}J = 0$ for all $j$. Solving these $n$ equations simultaneously will give the optimal points $\\theta$, and for squared error loss we know this will be the unique minima in a bowl-like surface. \n",
    "\n",
    "Omitting the derivation, there a linear algebra representation of the solution is \n",
    "\n",
    "\\begin{align}\n",
    "\\theta = (X^TX)^{-1}X^Ty,\n",
    "\\end{align}\n",
    "\n",
    "where $\\theta = (\\theta_0,...\\theta_n)$ and $y=(y_1,...y_m)$ are column vectors in the usual fashion whereas $X$ is a matrix we have formed from our feature values, $X_{ij} = x^{(i)}_j$. This matrix looks like a table of our data (but don't forget to include $x_0=1$).\n",
    "\n",
    "The matrix $X^TX$ is an $nxn$ matrix, which means for large numbers of features this is an increasingly infeasible computation - a good rule of thumb is above $n=10000$ start to consider gradient descent. \n",
    "\n",
    "Rarely you may find that $X^TX$ is non-invertible. This could happen if you have dependent (redundant) features like square footage of house in feet and in meters. In this case we shouldn't expect a unique minimizer since you will be able to alter the coefficients of those two features in tandem without changing $h_\\theta(x)$. Another reason is if you have more features than data points ($n > m$) - regularization is a technique for dealing with this situation that is covered later in the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span> <span style=\"background-color:#FFFFbb\">For multivariate linear regression with squared error cost function, there is an analytical solution for finding the optimal $\\vec{\\theta}$ called the Normal Equation. This method involves computing an $nxn$ matrix so it becomes less suitable for number of features greater than 10,000. You can occasionally be unable to invert the aforementioned matrix, which occurrs if two or more features are dependent (redundant) or if you have more features than data points.</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Week 3: Logistic Regression\n",
    "\n",
    "## Classification and Representation\n",
    "> Classification Problem: The variable $y$ that you want to predict takes on category-like discrete values, such as classifying an email as spam or not spam, or a transaction as fraudelent or not fraudulent (these are binary examples, but you can have multiclass).\n",
    "\n",
    "In binary classification your $y$ data just consists of 0s and 1s, but you could still try doing linear regression on these binary values to get a straight line hypothesis, and then set a **threshold** value in $y$ which you use to classify new predictions as 0 or 1. This doesn't usually work very well because the best fit line is sensitive to data points far out on the x-axis, and it seems counterintuitive to have a hypothesis that can output predictions much much different from real possibilities of 0 and 1. Logistic regression is an alternative algorithm for this type of problem, which will give us a hypothesis $h(\\theta)$ which always outputs in the range $[0, 1]$.\n",
    "\n",
    "A good form for a hypothesis that will always output on $[0, 1]$ is the sigmoid-shaped logistic function (hey, that's why it's called logistic regression!): \n",
    "\n",
    "\\begin{align}\n",
    "g = \\frac{1}{1+\\exp(-z_{\\theta}(x))},\n",
    "\\end{align}\n",
    "\n",
    "where $z = \\theta^Tx$ (which is our continuous linear regression model). This has the effect to take our linear regression outputs which can range from $(-\\infty, \\infty)$ and map them down in a smoothly varying way into the range $[0, 1]$. This leads to a nice inerpretation that the **logistic hypothesis is outputting a number which quantifies the probability that the observation $x$ has a corresponding $y$-value of 1**. Note that running the ML algorithm will still amount to selecting specific $\\theta$ values for this function.\n",
    "\n",
    "If you need to choose one or the other category, then given this probabilistic interpretation of the hypothesis output you might **choose $y=$1 when $h_{\\theta}(x) > 0.5$**. This will occur when your linear regression expression is positive: $z = \\theta^Tx > 0$. In the case of one feature it's obvious that $\\theta^Tx$ goes from negative to positive at the x-intercept for $z_\\theta(x)$. If instead you have two features then you can imagine that $z(\\theta)$ is a surface over the 2D plane $(x_1, x_2)$ and so it will intersect the plane $z=0$ along some line or curve in $(x_1, x_2)$ space. Setting $z = 0$ will give you exactly the equation for this line, and this line separates the two half-spaces in $\\theta$ space in which the logistic hypothesis should assign 1 and 0. \n",
    "\n",
    "> **Decision Boundary**: the curve (point for one feature, surface for 3 features etc.) in $\\vec{x}$ space that divides the two half-spaces in which the logistic hypothesis will predict 0 versus 1. The location of the boundary is completely determined by the value of the parameters $\\vec{\\theta}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span> <span style=\"background-color:#FFFFbb\">Logistic regression is used in classification problems to predict $y$s that take discrete category-like values. For binary categories we choose a form of hypothesis that always outputs between 0 and 1: $h_\\theta(x) = \\frac{1}{1+\\exp(-\\theta^Tx)}$, where $\\theta^Tx$ is the typical linear regression form. We interpret the output of this hypothesis as the probability that an $x$ has corresponding $y=1$, and so we \"predict\" $y=1$ when $h_\\theta(x) > 0.5$. The decision boundary for a hypothesis is the curve (point for one feature, surface for 3 features etc.) in $\\vec{x}$ space that divides the two half-spaces in which the logistic hypothesis will predict 0 versus 1.</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model\n",
    "How to actually fit (i.e. choose values of $\\theta$ based no our training set) a model of the logistic form discussed above? Recall in linear regression we minimized a cost function:\n",
    "\n",
    "\\begin{align}\n",
    "J(\\theta) = \\frac{1}{m}\\sum_1^m\\frac{1}{2}\\textrm{cost}(h_\\theta(x_i), y_i),\n",
    "\\end{align}\n",
    "\n",
    "where cost$(h(x_i), y_i)$ was the euclidian distance between the predicted and actual y values. If we tried to do this with the logistic form of $h_\\theta$ then it is non-convex and can have local minima. Instead **we define a different \"cost\" which satisfies:**\n",
    "- when true $y$ is 1, cost increases from 0 to infinity logarithmically as $h$ goes from 1 to 0\n",
    "- identical but reversed behavior when true $y$ is 0\n",
    "- thus zero cost when $h(x) = y$ for either 0 or 1 and some moderate cost when $h(x)=0.5$ for either 0 or 1\n",
    "\n",
    "<img src=\"images/pic7.png\" width=550></img>\n",
    "\n",
    "Since $y$ is always 0 or 1, the above behavior can be captured by:\n",
    "\n",
    "\\begin{align}\n",
    "\\textrm{cost}(h_\\theta(x), y) = -y\\log(h_\\theta(x)) - (1-y)\\log(1-h_\\theta(x)).\n",
    "\\end{align}\n",
    "\n",
    "This can now be plugged into $J$ to give the full cost function which our ML algorithm should minimize for logistic regression (as a side note, this procedure is the maximum likelihood estimator for the logistic model). Just as in linear regression, we can use gradient descent to actually implement the algorithm for finding the minimizing $\\theta$, and actually **the $\\theta_j$ GD \"update\" rule using the logistic cost function looks identical to the one we got for least squares**:\n",
    "\n",
    "\\begin{align}\n",
    "\\theta_j := \\theta_j - \\alpha\\frac{1}{m}\\sum^m_{i=1}\\big(h(x^{(i)})-y^{(i)}\\big)x^{(i)}.\n",
    "\\end{align}\n",
    "\n",
    "There are other algorithms besides gradient descent for minimizing functions, in all cases the code relies on computing $J(\\theta$) and $\\frac{\\partial}{\\partial \\theta}J(\\theta)$. These algorithms usually converge faster than simple GD but they are more complex and so you need to make sure you use a library with a good implementation of them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Classification\n",
    "In the above we fit a \"straight\" line which was the the decision boundary to separate our $x$-space into two halves in which data would be classified as 0 or 1. We did this by using the logistic form of hypothesis $h_\\theta(x) = \\frac{1}{1+\\exp(-\\theta^Tx)}$ which involved the simple linear regression quantity $\\theta^Tx$. But often our $y$-values can take on several labels, not just 0 versus 1.\n",
    "\n",
    "One approach to dealing with is called **\"One vs. Rest\"** which says if e.g. you have three possible lables $\\{1, 2, 3\\}$, you can split the problem into three different binary classification problems: *1 or not 1*, *2 or not 2*, and *3 or not 3*. To excecute, for example, *1 or not 1* you take all the training data with labels 2 and 3 and map them to a label 0, then you execute the standard binary logistic regression on this data to get a hypothesis $h^{(1)}_\\theta(x)$ whose decision boundary divides category 1 from the rest of the space. Rinse and repeat to get  $h^{(2)}_\\theta(x)$ and  $h^{(3)}_\\theta(x)$. Finally, **to classify a data point $x$ you assign it to whichever of the three classes gives the largest  $h^{(i)}_\\theta(x)$.** \n",
    "\n",
    "<img src=\"images/pic8.png\" width=550></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span> <span style=\"background-color:#FFFFbb\">The logistic hypothesis is \"fit\" by minimizing some cost function like in linear regression. For this hypothesis form, there is a convex cost function with the following behavior: when the true $y$ is 1, cost increases from 0 to infinity logarithmically as $h$ goes from 1 to 0, and vice versa for the true $y$ being 0. Multiclass classification can be done by splitting a $k$-categories problem into $k$ binary problems of *catgory $i$ or not $i$*, each of which is a standard binary logistic regression returning a hypothesis $h^{(i)}_\\theta(x)$. You then classify new data according to whichever  $h^{(i)}_\\theta(x)$ gives the highest value - this is called \"One-versus-Rest\".</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week3: Regularization\n",
    "The more features you add to your hypothesis, such as using a high order polynomial, the better the hypothesis can do at hitting all the training set points almost exactly ($J \\rightarrow 0$). But usually such complicated functions are not accurate reflections of the underlying physical process and so the hypothesis won't generalize to new data - it is **overfitted**. \n",
    "\n",
    "<img src=\"images/pic9.png\" width=550></img>\n",
    "\n",
    "On the other extreme if you have too few features to capture the shape of the relationship the model is **underfit** and is said to have high *bias*, as in it is biased to predict certain $y$ values even in the presence of contradicting training set data. Logistic regression is also subject to overfitting in which you can get very convoluted decision boundaries. \n",
    "\n",
    "Two options for dealing with overfitting are:\n",
    "1. Manually discard some features (or use a model selection algorithm to do so - these are covered later)\n",
    "2. Use regularization, which will keep all features but reduce their magnitude.\n",
    "\n",
    "Regularization works well when you have a lot of features, each of which contributes a little bit in producing the $y$-value. **In regularization you add a penalty to your cost function which is large when the coefficient parameters for certain features are large**. This causes the fit to tend to return a hypothesis in which those parameters are small, and thus it has a smoother and simpler shape for the relationship between $x$ and $y$.\n",
    "\n",
    "In fact, penalizing all features to drive every $\\theta_i$ except $\\theta_0$ to zero is a good approach to obtain a simple fitted relationship - it has the effect of biasing your hypothesis towards the constant $\\theta_0$. This can be done with a penalty in the cost function like:\n",
    "\n",
    "\\begin{align}\n",
    "J_{LSQ}^{reg}(\\theta) = \\frac{1}{m}\\bigg[\\sum_1^m\\frac{1}{2}(h(x_i)-y_i)^2 + \\lambda\\sum_1^n\\theta_j^2\\bigg].\n",
    "\\end{align}\n",
    "\n",
    "The two terms in the expression capture two goals - to fit the training set well, and to find a simple, smooth relationship that will generalize to new data well. But note if your $\\lambda$ is too large you will wind up underfitting ($h_\\theta(x) \\approx \\theta_0$).\n",
    "\n",
    "For linear regression the gradient descent update rule and the analytical Normal Equation solution can both be modified to account for this new form of the cost function. In fact, with the regularization term the relevant matrix quantity in the normal equation is guaranteed to be invertible. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span> <span style=\"background-color:#FFFFbb\">In linear and logistic regression hypotheses with more features will always \"hit\" the training set values more closely, but often are convoluted and wiggly functions that don't represent the underlying physical relationship and won't generalize well to new data. Regularization is adding a penalty term to the cost function which penalizes large values of certain (or commonly all) parameters, resulting in small-parameter hypotheses that represent a smoother and simpler relationship. Another option for dealing with overfitting is to manually discard some features, or use a model selection algorithm to do so.</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week4: Neural Networks Representation\n",
    "## Motivation for Neural Networks\n",
    "When the number of variables $n$ is large, all the polynomial transforms of those features (including cross terms) grows like $n^2$ and so it is computationally difficult to fit complex models with non-linear terms. Consider classifying an image as *car* vs. *not car*. Each data point, $x$, is an image of identical size/resolution which corresponds to a list of pixel intensity values (each pixel is a feature). For a 50x50 pixel image, a model including all terms up to quadratic power would have 3 million features!\n",
    "\n",
    "Interesting Aside: It seems from experiment that any piece of human cortex can learn to interpret input from any \"sensor\" (meaning sensory neurons like retinal, auditory, etc.) it is wired to. This lead to the hypothesis that to have software mimic all the disparate brain functionality you may actually only need one algorithm which knows how to \"learn\" a function.\n",
    "\n",
    "## Mathematical Representation\n",
    "We model a single neuron as a unit which receives multiple inputs $\\vec{x}$ and produces a single output according to the logistic sigmoid hypothesis $h_\\theta(x) = \\frac{1}{1+\\exp(-\\theta^Tx)}$ where the $\\vec{\\theta}$ is often called the \"weights\". A *layer* is a set of such units, each with it's own weights. A *network* is a formed by stacking these layers such that the outputs of one layer are the inputs to the layer above it. The real input data are the bottommost layer, the output of the topmost layer is the final output of the model, and intermediate layers are called *hidden layers*. Each interface between a layer is represented by a matrix which gives the \"weight\" on the output from neuron $x_i$ in the lower layer feeding into neuron $x_j$ in the top layer. Some mathematical defintions:\n",
    "\n",
    "- $a_j^{(i)}$ is the *activation* (output value) of the $j^{th}$ neuron in the $i^{th}$ layer\n",
    "- $\\Theta^{(i)}$ is the matrix holding the weights from layer $i$ into layer $i+1$. Its rows represent neurons in the $i+1^{th}$ layer.\n",
    "\n",
    "The *activation* of a unit is computed from the logistic function $g(z) = \\frac{1}{1+\\exp(-z)}$, where $z = \\theta^Tx$. So formally for a single neuron we have:\n",
    "\n",
    "\\begin{align*}\n",
    "a_j^{(i)} = g\\big(\\Theta^{i-1}_{j0}a^{(i-1)}_0 + \\Theta^{i-1}_{j1}a^{(i-1)}_1 + \\cdots \\big) = g(z^{(i)}_j).\n",
    "\\end{align*}\n",
    "\n",
    "To vectorize the notation, the $z$ values of all the different units in layer $i$ can be combined into the vector $z^{(i)}$ and then the activations of these units can be represented by the vector $a^{(i)} = g(z^{(i)})$ where the sigmoid function is understood to apply element-wise. **For the $i^{th}$ layer, think of $a^{(i)}$ as its output, $z^{(i)}$ as its weighted input, and $g$ as the computation that it does on its weighted input to generate its output. The weighted input is the previous layer's output $a^{(i-1)}$ weighted by that interface $\\Theta^{(i-1)}$.**\n",
    "\n",
    "You can imagine constructing, for example, the output of the third layer: \n",
    "\n",
    "\\begin{align*}\n",
    "a^{(3)} = g(z^{(3)}) = g\\big(\\Theta^{(2)}a^{(2)}\\big) = g\\big(\\Theta^{(2)}g(z^{(2)})\\big) = g\\big(\\Theta^{(2)}g(\\Theta^{(1)}x)\\big).\n",
    "\\end{align*}\n",
    "\n",
    "By convention we include a **bias unit** $a_0^{(i)}$ for every layer that outputs a constant value, including one for the first layer where it is just like including a column of all ones for $x_0$ in linear regression to multiply the intercept parameter $\\theta_0$. The topmost layer is typically a single neuron and it's output is our real hypothesis output, $a^{(\\textrm{top})} = h_\\Theta(x)$, and the hypotheses are now parameterized by the set of weighting matrices $\\Theta = \\{\\Theta^{(i)}\\}$.\n",
    "\n",
    "\n",
    "<img src=\"images/pic12.png\" width=550></img>\n",
    "\n",
    "Here is an interesting interpretationfor what a neural network is doing: considering only the top layer we appear to be doing just a standard logistic regression, where the features of your model are the activations of the second highest layer. **So you can imagine that the underlying layers are just adding a step to logistic regression where you first \"learn\" what features you should use other than $x$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span> <span style=\"background-color:#FFFFbb\"> A single neuron (a unit) receives multiple inputs, weights them by $\\vec{\\theta}$, and produces a single output according to the sigmoid function $h_\\theta(x) = \\frac{1}{1+\\exp(-\\theta^Tx)}$. A *layer* is a set of such units, each with it's own weights. A *network* is formed by stacking these layers such that the outputs of one layer are the inputs to the layer above it. The real input data are the bottommost layer, the output of the topmost layer is the final output of the hypothesis, and intermediate layers are called *hidden layers*. The relevant quantities can be vectorized for each layer: For the $i^{th}$ layer, think of $a^{(i)}$ as its output, $z^{(i)}$ as its weighted input, and $g$ as the computation that it does on its weighted input. The weighted input is the previous layer's output $a^{(i-1)}$ weighted according to the matrix $\\Theta^{(i-1)}$. Considering only the top layer we appear to be doing just a standard logistic regression, where the features of your model are the activations of the second highest layer. So you can imagine that the underlying layers are just adding a step to logistic regression where you first \"learn\" what features you should use other than $x$.</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications and Examples\n",
    "As a first example, consider input data $x=\\{x_1, x_2\\}$ where each can be either 0 or 1, and we are doing binary classification where our $y$-values also take on either 0 or 1. Make a network with no hidden layers so the $x$ just feeds into a single neuron, and let the weights be such that the activation (output) of this one neuron is $h_\\theta(x) = g(-30 + 20x_1 + 20x_2)$. You can check the four cases for $\\{x_1, x_2\\}$ and see that the output is $h\\approx 0$ except for when both $x$s are 1 and it is $h\\approx 1$. This means our network can represent a hypothesis that is essentially the AND function. Other simple logical operations can also be represented by single-layer networks. More complicated operations can be represented by including hidden layers to execute intermediate steps of the logic.\n",
    "\n",
    "<img src=\"images/pic13.png\" width=550></img>\n",
    "\n",
    "Multiclass classification can be done with a neural network whose top layer has as many outputs as the number of possible labels. Each output is interpreted similarly to the One-vs-Rest approach to be the probability of $y$ being label $i$ versus not label $i$. In this case $y$ is more conveniently labeled by a vector, for example with three possible labels, a training data point that we know falls into the first category would have $y = [1, 0, 0]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week5: Neural Networks Fitting\n",
    "## Cost Function and BackPropagation\n",
    "Our net has $L$ layers, each layer has $s_l$ number of units in it (not counting bias unit) and for classification problems we want our topmost output layer to have $K$ units in it, where $K$ is the number of possible labels. The output of our model is then a vector in $\\cal{R}^K$ and each training point has a $y$ value in the same space, but pointed entirely along one dimension like $[1, 0, 0]$.\n",
    "\n",
    "For fitting any model you first need to specify the objective or cost function, the thing which the algorithm should seek to optimize in the parameter space of the model. Previously in this class we started with a really common least squares cost function for linear regression, and a somewhat different logarithmic one for binary logistic regression, and both of these cost function we later modified to include a regularization term (complexity penalty) to help prevent overfitting. For neural nets we use a cost function which is a generalization of the one for logistic regression. For a single training data point, $(\\vec{x},\\vec{y})$, it computes the conventional log-loss element-by-element for each of the $K$ elements of $\\vec{y}$ and then sums these. \n",
    "\n",
    "Recall the conventional *cost* function for a single data point in binary logistic regression is:\n",
    "\n",
    "\\begin{align}\n",
    "\\textrm{cost}(h_\\theta(x), y) = -y\\log(h_\\theta(x)) - (1-y)\\log(1-h_\\theta(x)).\n",
    "\\end{align}\n",
    "\n",
    "So that the full loss function for conventional binary logistic regression is a sum of these costs over the training set whose data points are indexed by $i$:\n",
    "\\begin{align}\n",
    "J(\\theta) = \\frac{1}{m}\\sum_{i=1}^m\\frac{1}{2}\\textrm{cost}\\big(h_\\theta(x^{(i)}), y^{(i)}\\big).\n",
    "\\end{align}\n",
    "\n",
    "Now for a neural net the above expression becomes $J_k(\\theta)$ and instead has $y^{(i)}_k$, which is the $k^{th}$ component of vector $\\vec{y}^{(i)}$ and $(h_\\theta(x^{(i)})_k$ which is the output of the $k^{th}$ neuron in our top layer. We compute this $J_k(\\theta)$ for each such component then sum them all. Note that here $\\theta$ stands in for all the parameters of the net, meaning the values of every element of every weighting matrix.\n",
    "\n",
    "\\begin{align}\n",
    "J_{\\textrm{neural}}(\\theta) = \\sum_{k=1}^KJ_k(\\theta) = \\sum_{k=1}^K\\bigg[\\frac{1}{m}\\sum_1^m\\frac{1}{2}\\textrm{cost}\\big(h_\\theta(x^{(i)})_k, y^{(i)}_k\\big)\\bigg].\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The regularization penalty for a neural net is designed to keep every element of every weighting matrix small, except the weights on the inputs from the bias units, $x_0$.** It has a very similar structure to how we regularized linear and logistic regression. \n",
    "\n",
    "\\begin{align}\n",
    "J_{\\textrm{neural}}(\\theta) = \\sum_{k=1}^K J_k(\\theta) - \\frac{\\lambda}{2m}\\sum_{k=1}^{K-1}\\bigg[\\sum_{i=1}^{s_l}\\sum_{j=1}^{s_{l+1}} \\big(\\Theta^{(l)}_{ji}\\big)^2\\bigg].\n",
    "\\end{align}\n",
    "\n",
    "When looking at the sum limits remember that for the activation of the bias unit is indexed by 0, so starting the sum at $j=1$ rather than $j=0$ means we are ommitting those elements and the bias bias weights don't get penalized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward vs. Back Propagation\n",
    "\n",
    "To use any kind of numerical technique to actually minimize the cost function, like gradient descent, we will need to calculate the value of $J(\\theta)$  and its partial derivatives in parameter space (remember, every weight $\\Theta^{(l)}_{ij}$ is an independent parameter).In order to do this we use two algorithms for computing values in a net - forward and back propagation.\n",
    "\n",
    "**Forward propagation** refers to simply applying the formulas that allow you to calculate the ouput layer's activation starting from the input layer (which is our `x`'s). For example:\n",
    "\\begin{align*}\n",
    "a^{(3)} = g(z^{(3)}) = g\\big(\\Theta^{(2)}a^{(2)}\\big) = g\\big(\\Theta^{(2)}g(z^{(2)})\\big) = g\\big(\\Theta^{(2)}g(\\Theta^{(1)}x)\\big).\n",
    "\\end{align*}\n",
    "\n",
    "To get $J(\\theta)$ you can first code up the forward propagation algorithm to get $h_\\theta(x)$ and then just feed it into the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the partial derivatives (ignoring the penalty term) we use a different algorithm called **Back propagation**. It turns out the partials $\\partial /\\ \\partial \\Theta^{(l)}_{ij}$ of the cost for a single training point can be expressed in terms of the activations $a^{(l)}$ and what are called the **errors** of the neurons:\n",
    "\\begin{align*}\n",
    "\\delta^{(l)}_j \\equiv \\frac{\\partial}{\\partial z^{(l)}_j}\\big(\\textrm{cost}_i\\big),\n",
    "\\end{align*}\n",
    "\n",
    "where $\\textrm{cost}_i$ refers to the cost of the $i^{th}$ training data point. Error is actually a pretty descriptive name - for a single neuron it captures how much change, or \"error\" there is in the cost if you were to change the input to that neuron just a bit, but keep everything else about the net fixed (of course all the downstream activations that depend on that neuron will change a bit in response). \n",
    "\n",
    "Imagine having just a single training point and using the $x$ value to forward propagated all the activations of each neuron in the net and then used the outputs $a^{(K)}$ and $y$ to compute the cost function. The cost function depends directly on the top layer so it's straightforward to write down how the cost will change if you perturb a top layer input $z^{(K)}_j$. If you instead perturb an input to the second-from-top layer, the change in the cost function can be calculated from the ensuing changes in the top layer inputs multipled by how much the cost changes per change in each top layer input. And so forth for deeper layers. We are speaking in terms of $z$'s, but as a final step you could multiply by how much an input changes with a change in $\\Theta^{(l)}_{ij}$. For instance,informally we could say:\n",
    "\n",
    "\\begin{align*}\n",
    "\\Delta(\\textrm{cost}) = \\frac{\\Delta(\\textrm{cost})}{\\Delta z^{(K)}}\\Delta z^{(K)} = \\frac{\\Delta(\\textrm{cost})}{\\Delta z^{(K)}} \\frac{\\Delta z^{(K)}}{\\Delta z^{(K-1)}}\\Delta z^{(K-1)} = \\frac{\\Delta(\\textrm{cost})}{\\Delta z^{(K)}} \\frac{\\Delta z^{(K)}}{\\Delta z^{(K-1)}} \\frac{\\Delta z^{(K-1)}}{\\Delta \\Theta^{(K-2)}_{ij}} \\Delta \\Theta^{(K-2)}_{ij}\n",
    "\\end{align*}\n",
    "\n",
    "This is how back propagation lets you churn out the errors by back-tracking from the output layer whose error can be calculated pretty directly (it turns out to be $\\delta^{(K)} = a^{(K)} - y$). I'm omitting the specific formulae for lower-layer errors because it is not particularly enlightening. \n",
    "\n",
    "To be clear, the forward and then back propagation algorithms are run on a single training point at a time, and then the full cost function and it's partials can be had by just summing up contributions from all the individual training points. Once you have an algorithm for getting the cost function and all it's partials at a point $\\theta$, you are in position to use one of several different numerical optimization algorithms, like gradient descent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A full algorithm for getting $J(\\theta)$ and it's partials at a point in parameter space $\\theta$ would be:\n",
    "\n",
    "- For each training point $(x, y)$:\n",
    "    - Use forward propagation to compute all the activations of the net for that input $x$\n",
    "    - Plug the top layer activations $h_\\theta(x) = a^{(K)}$ into the cost function to get the cost for that training point\n",
    "    - Use back propagation and the computed $a^{(K)}$ to compute all the errors for that training point\n",
    "    - Use all the computed errors and activations to calculate the contribution to each of the partials from that training point \n",
    "- Sum the costs of the training points to get the cost function at $\\theta$\n",
    "- Sum the contributions of the training points to each partial to get each complete partial at $\\theta$\n",
    "- For the full cost, add in the regularization term which just depends on the $\\Theta^{(l)}_{ij}$'s\n",
    "- For the complete partials, add in the piece from the regularization term $\\lambda \\Theta^{(l)}_{ij}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span> <span style=\"background-color:#FFFFbb\"> For classification problems we use a net whose top layer has $K$ units where $K$ is the number of possible labels, so the output of the model and our data $y$ values live in $\\cal{R}^K$. The cost function of a single point $(\\vec{x},\\vec{y})$ computes the conventional log-loss element-by-element for each of the $K$ elements of $\\vec{y}$ and then sums these. The loss $J(\\theta)$ is the average over the individual point costs, per usual. To compute a single point cost we first use forward propagation: applying the formulas that allow you to calculate the each layer's activation starting from the input layer (which is our `x`'s). To compute the partials of single point cost we use back propagation: applying formulas to calculate each layer's error starting from the error of the top layer. The error of a neuron is $\\delta^{(l)}_j \\equiv \\partial /\\ \\partial z^{(l)}_j\\big(\\textrm{cost}_i\\big)$. The partials of the single point cost just depend on these errors and the activations, and the partials of the full loss are simple sums of these contributions from single point costs. Regularization can be included to keep $\\Theta^{(l)}_{ij}$ small, excluding the weights on bias units.\n",
    " </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation in Practice\n",
    "\n",
    "Backprop implementation is susceptible to subtle bugs that can inhibit performance even though gradient descent may appear to be working ($J$ decreasing every iteration). **Gradient Checking** makes a first order approximation to a partial derivative of $J$ with respect to one of the parameters $\\Theta^{(l)}_{ij}$ as a sanity check. Wherever you are in parameter space, you simply move out along that parameter's dimension a small distance $\\epsilon$ to the left and right of your original point, and compute:\n",
    "\\begin{align*}\n",
    "\\frac{\\partial}{\\partial \\Theta^{(l)}_{ij}}J(\\theta) \\bigg|_{\\theta_0} \\approx \\frac{J(\\theta_0+\\epsilon)-J(\\theta_0-\\epsilon)}{2\\epsilon}\n",
    "\\end{align*}\n",
    "\n",
    "Regarding initialization, it turns out that if you begin trying to run gradient descent at the point $\\Theta^{(l)}_{ij} = 0 \\ \\forall \\ i,j, l$ or any other symmetric set of weights, then gradient descent will only ever travel along a line in parameter space that keeps keeps every unit within a layer computing the exact some activation. This obviously limits the power of the neural net model, since your top layer will essentially only ever see one unique features. The alternative is to randomly initialize each weight to a small value close to 0. \n",
    "\n",
    "Regarding choosing a net architecture, the number of input units will be the number of features, and the for multiclass classification the number of output units will be the number of labels. A good default choice is a single hidden layer, or if more than one then each hidden layer having the same number of units. Generally the more units in a hidden layer the better (though it will become more computationally difficult) - the number might range from the same as the number of input features to twice or even three or four times that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span> <span style=\"background-color:#FFFFbb\"> Backprop is susceptible to subtle bugs, it is helpful to compute the first order numerical approximation $\\frac{\\partial}{\\partial \\Theta^{(l)}_{ij}}J(\\theta) \\bigg|_{\\theta_0} \\approx \\frac{J(\\theta_0+\\epsilon)-J(\\theta_0-\\epsilon)}{2\\epsilon}$ to compare with your backprop results at first. For the fitting algorithm make sure to initialize all the weights to small random number close to 0. Finally, the rules of thumb for choosing a net architecture are: the number of input units will be the number of features, for multiclass classification the number of output units will be the number of labels, use single hidden layer, or if more than one then each hidden layer having the same number of units, and the number of hidden units per layer might range from the same as the number of input features to twice or even three or four times that.\n",
    " </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week6: Advice for Applying Machine Learning\n",
    "\n",
    "## Evaluating a Learning Algorithm\n",
    "How do you iterate when you find your fitted ML model doesn't do well on new data?\n",
    "- Collect more training examples (*hard work - don't do this prematurely*)\n",
    "- Restricting to a small number of the most important features to avoid overfitting\n",
    "- Collecting more data to get new features (*hard work - don't do this prematurely*)\n",
    "- Constructing more complex features from existing data\n",
    "- Increasing or decreasing regularization $\\lambda$\n",
    "\n",
    "Since two of these things are effort intensive, we seek some **diagnoistics that will tell us where our ML algorithm is falling down and guide our next steps.** As a precursor, we want to evaluate how the fitted ML model is actually doing.\n",
    "\n",
    "### Evaluating Performance of Your Algorithm\n",
    "When evaluating a hypothesis, for large number of features it is hard to visualize a decision boundary and eyeball whether you are overfitting. **An alternative check we can use is to split your data into a chunk to use for fitting (\"training set\"), and a chunk to use for testing the fitted model (\"test set\").** We hope that the error on the test set, $J_{\\textrm{test}}(\\theta)$, is small once we have fitted for the optimal $\\theta$ by minimizing $J_{\\textrm{train}}(\\theta)$. In classification problems it is also common to consider the misclassification error (1 for correct assignment, 0 otherwise) instead of $J_{\\textrm{test}}(\\theta)$. \n",
    "\n",
    "This evaluation method can help you with model selection also, such as what features to include and what regularization strength to use. **One way to interpret this use of test error is that you are actually fitting a meta parameter of your model dictating e.g. what order of polynomial to use.** Consider doing linear regression with a polynomial formed hypothesis, e.g. $h_\\theta(x) = \\theta_0 + \\theta_1x + \\cdots \\theta_d x^d$. We could fit and then compute $J_{\\textrm{test}}$ for the possible hypotheses up to maybe $d=7$, and then choose the one that did the best on our test set. But this still results in overly optmistic idea of how well the model will generalize, because we essentially used the test set to fit the meta parameter $d$. That is, we expect the final selected model to have done better on the test set than on some other random data sample because we selected it entirely on the basis of *doing* well on the test set.\n",
    "\n",
    "The best solution to avoid overfitting the parameters of potential hypotheses as well as to get a sense of how well the final fitted hypothesis might perform is to split your data in three: **a training set to fit hypotheses, a cross-validation set to \"fit\" meta parameters (model selection), and a test set to evaluate the performance of the final model.** A common split is of data points is 60% / 20% / 20%, respectively. The procedure is as follows:\n",
    "- For each candidate hypothesis form in your model family, fit it's parameters by minimizing $J_\\textrm{train}$\n",
    "- For each fitted hypothesis form compute $J_\\textrm{cross-val}$ and select the hypothesis which minimizes it\n",
    "- Compute $J\\textrm{test}$ on this final selected hypothesis as a real measure of the generalization error "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span> <span style=\"background-color:#FFFFbb\"> An overfit model does very well on the training data which was used to fit it, but doesn't generalize well. To guard against overfitting, you can consider a variety of forms of hypotheses, like different degree polynomials, and fit them all with a \"training\" subset of the data, then test the fitted forms on the remaining \"cross-validation\" subset. However, this can be interpreted as using the \"cross-validation\" data to fit a meta parameter of the model (model selection), so $J_\\textrm{cross-val}$ is not an accurate reflection of expected future performance. Instead use a final reserved \"test\" subset of the data to compute a realistic estimate of generalization error, $J_\\textrm{test}$. A common data split is 60/20/20, respectively. \n",
    " </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias vs. Variance\n",
    "When your model doesn't generalize well you most likely have an underfitting problem (high bias) or overftting problem (high variance), and identifying which one can guide how you modify your approach.\n",
    "\n",
    "In underfitting, your procedure is not sufficiently responsive or sensitive to training data - instead it has a \"bias\" towards always landing on the same hypothesis regardless of the training data. This could be that the form of the model is not sufficiently complex so that it's simply not capable of capturing relationships among the input features. Another possibility is that you are regularizing too strongly and forcing the fit towards some constant. \n",
    "\n",
    "In overfitting your procedure is instead *too* sensitive to the training points and so the specific fitted hypothesis can vary wildly with small changes in the training set. This can result from a model form that is very complex and thus tends to capture behavior in the training data that is in reality just random noise. \n",
    "\n",
    "To recognize which case you have, we typically expect:\n",
    "- Underfitting (High Bias) : $J_\\textrm{train}$ is high, and $J_\\textrm{cross-val} \\approx J_\\textrm{train}$.\n",
    "- Overfitting (High Variance) : $J_\\textrm{train}$ is low, and $J_\\textrm{cross-val} \\gg J_\\textrm{train}$\n",
    "\n",
    "\n",
    "<img src=\"images/pic14.png\" width=550></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that regularization penalizes complexity by pushing parameter values $\\theta_i \\rightarrow 0$ for $i\\neq 0$. A very strong regularization parameter will thus result in choosing an underfit hypothesis, while too small a $\\lambda$ will not allow regularization to perform it's job of preventing overfitting. **To choose a good regularization parameter, you can treat $\\lambda$ as a meta parameter in your model and execute the procedure above to select it's value by minimizing $J_\\textrm{CV}$.** But note, here you would fit individual hypotheses using the regularized form of $J$, but then $J_\\textrm{CV}$ should be computed just with the squared error (or similar) component. \n",
    "\n",
    "\n",
    "<img src=\"images/pic15.png\" width=550></img>\n",
    "\n",
    "### Learning Curves\n",
    "Learning curves serve as a sanity check that your algorithm is working correctly and also to help investigate potential bias or variance problems. Imagine you have split your data into training and CV subsets. How will $J_\\textrm{train}$ and $J_\\textrm{CV}$ change if you were fit a specific hypothesis starting with just a single point in the training set and then  progressively using more points in the training set to fit? ($J_\\textrm{CV}$ is calculated on the full CV set each time). Since it is easy to fit on one or two points we expect $J_\\textrm{train}$ to start low, but then it should increase as we add more points into the fitting procedure and hopefully begin to flatten out. On the other hand, since we are probably not getting a very *good* estimate of parameters by using only one or two training points, we expect $J_\\textrm{CV}$ to start high, and hopefully decrease as we use more and more training points for the fit. **This plot of $J_\\textrm{train}$ (calculated on the truncated training set) and $J_\\textrm{CV}$ as a function of $m_\\textrm{train}$ is a Learning Curve.**\n",
    "\n",
    "<img src=\"images/pic16.png\" width=550></img>\n",
    "\n",
    "Now if your procedure has high bias, then it can't support a sufficiently complex decision boundary *regardless* of how many points you use in fitting - thus $J_\\textrm{CV}$ tends to flatten out quickly and not decrease with increasing $m_\\textrm{train}$. $J_\\textrm{CV}$ will also probably flatten to a value close to $J_\\textrm{train}$ since high \"bias\" means the model's output is relatively insensitive to the particular set of input points. Conversely, overfitting manifests as a large $J_\\textrm{CV}$ which decreases slowly with $m_\\textrm{train}$, though it will look like the two losses would converge if extrapolated out to very large number of training points.\n",
    "\n",
    "In light of this discussion we can revisit the menu of our options when our algorithm underperforms:\n",
    "- High Variance (Overfitting): collect more training examples, restrict to fewer features, or increase regularization\n",
    "- High Bias (Underfitting): collect more data / execute transforms to get new features, or decrease regularization\n",
    "\n",
    "For neural networks a large number of hidden layers and hidden units per layer makes it susceptible to overfitting, although it is still often preferable to include strong regularization rather than restricting to a smaller net. For neural nets you can treat the number of hidden layers as a meta parameter and \"fit\" it using the \"train\"/\"cross-val\" data split procedure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span> <span style=\"background-color:#FFFFbb\"> In underfitting (high bias) we expect $J_\\textrm{train}$ is high and $J_\\textrm{cross-val} \\approx J_\\textrm{train}$. In overfitting (high variance) we expect $J_\\textrm{train}$ is low and $J_\\textrm{cross-val} \\gg J_\\textrm{train}$. A Learning curve is a plot of $J_\\textrm{train}$ (calculated on a truncated training set) and $J_\\textrm{CV}$ (calculated on the full cross-val set) as a function of $m_\\textrm{train}$ (the size of the truncated training set). In Learning Curves underfitting manifests as $J_\\textrm{CV}$ flattening out quickly at something close to $J_\\textrm{train}$, while overfitting manifests as a large $J_\\textrm{CV}$ which decreases slowly, with a large gap from $J_\\textrm{train}$ though it will look like the two losses would converge if extrapolated out. For overfitting: collect more training examples, restrict to fewer features, or increase regularization. For underfitting: collect more data or execute transforms to get new features, or decrease regularization.</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week6: Machine Learning System Design\n",
    "In working on a ML system there are lots of things you *could* spend time on, like gather more training data versus coming up with new features etc. Error analysis is to help us choose what is the best use of our time. A recommended approach is:\n",
    "- very quickly prototype a simple algorithm and run it on the CV data to evaluate performance\n",
    "- plot learning curves to identify bias or variance, this dictates what actions might \"help\" performance\n",
    "- **error analysis**: manually examine the mistakes your algorithm made on the CV data\n",
    "\n",
    "Regarding error analysis, **categorize the data points that the algorithm fails on - high occurrence categories deserve more attention in designing features which identify them. Similarly, identify different potential types of features in these failed points, and work on improving your representation of those features with high occurence.** For instance, a spam classifying email may fail most frequently on phishing emails, or failed data points may contain a lot of unusual punctution - these are then both worth more attention. \n",
    "\n",
    "You often want to try different variations of your algorithm, such as should the spam classifier use word \"stemming\" or not, so it is good to have a single error metric like CV error percentage that you can use to compare performance. \n",
    "\n",
    "## Skewed Data\n",
    "One situation where it is hard to develop a good error metric is \"skewed\" class - when one class is heavily overrepresented in your data. For instance, in cancer diagnoses if only 0.5% of your patient data points are cancer positive, then always predicting \"negative\" for cancer will give you a 99.5% success rate! **In such a binary case with skewed representation two more useful metrics are precision and recall.** Assuming we give the rare class label $y=1$, they are defined as:\n",
    "\\begin{align*}\n",
    "\\textrm{precision:} \\qquad \\textrm{P}(y=1\\  \\big|\\  h_\\theta(x)=1) = \\frac{\\textrm{no. correct positive}}{\\textrm{no. predicted positive}}\\\\\n",
    "\\\\\n",
    "\\textrm{recall:} \\qquad \\textrm{P}(h_\\theta(x)=1 \\ \\big| \\  y=1)  = \\frac{\\textrm{no. correct positive}}{\\textrm{no. with condition}}\n",
    "\\end{align*}\n",
    "\n",
    "Unfortunately you typically have to trade off between precision and recall. If you want to limit the number of false positives (push the precision toward 1) then you can accomplish this by changing the boundary for what $h_\\theta(x)$ values you map to 0 versus 1, for example $h_\\theta(x) > 0.7 \\rightarrow y=1$, but this will result in worse recall. The reverse situation occurs when you want to limit false negatives and you choose e.g. $h_\\theta(x) > 0.3 \\rightarrow y=1$. As you vary the details of your classifier you trace out a curve of precision vs. recall whose exact shape depends on the nature of your classifier. **The F-score is a way to map the precision and recall to a single real number for rapidly comparing different algorithm's performance:**\n",
    "\\begin{align*}\n",
    "F_1 = 2\\frac{PR}{P+R}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large Data Sets\n",
    "A famous study showed that between several common algorithms the performance on a given task was not very different, but that every algorithm got significantly better when give more and more training data. The conclusion is that there are algorithms for which it is useful to gather as much training data as you can. \n",
    "\n",
    "First assume that our features $x$ actually contain enough information to accurately predict $y$ - a good test is whether a human expert could make a prediction with confidence. A counterexample is predicting housing prices just from square footage. Now consider using an algorithm like linear regression with many parameters or a neural net - **these are \"low bias\" algorithms as they can fit complex functions and so normally we would worry about overfitting (high variance), but if you use a massive enough training set then this prevents overfitting.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span> <span style=\"background-color:#FFFFbb\"> Quickly prototype simplest algorithm and plot learning curves to analyze for bias or variance - let that guide your next steps. Also carefully inspect data points where the algorithm failed and use those to inform the next iteration regarding e.g. feature design. For quickly comparing algorithm performance use a single real number such as misclassification rate. For classification where one or more classes are very overrepresented instead use the F-score computed from the Precision ($\\textrm{P}(y=1\\  \\big|\\  h_\\theta(x)=1)$) and Recall ($\\textrm{P}(h_\\theta(x)=1 \\ \\big| \\  y=1)$). There is a trade-off between these two, which you realize by shifting the threshold for your classifier. In cases where your features contain enough information (a human expert would consider them sufficient for prediction) and you are using a complex (high variance) model like a neural net or regression with many features, then training on increasingly large data sets can help prevent overfitting.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Week7: Support Vector Machines\n",
    "Often within supervised learning the particular algorithm is much less important than skill in training it properly, designing features, and giving it more data. SVM is a simple classification algorithm that can learn complex non-linear hypothesis (like a neural net) that can support new feature creation, via kernel functions on $x$, in a computationally cheap way.\n",
    "\n",
    "## Large Margin Classification\n",
    "The simplest SVM can be constructed by modifying the logistic regression procedure. Recall the common log loss cost function in logistic regression: if the true $y = 1$, then the log loss cost function grows very large for $\\theta^Tx \\ll 0$ and is asymptotically zero for $\\theta^Tx \\gg 0$ (the reverse holds when $y = 0$). **In SVM algorithms the log loss cost function is replaced by a simpler piecewise curve which is a flat line in the asymptotically zero region joined to a straight line with a finite slope. The transition points are $z=1$ (for the y=1 curve) and $z=-1$ (for the y=0 curve). This is called the \"hinge loss\". Another major difference from logistic regression is that an SVM hypothesis outputs just a binary 0 or 1 whereas logistic regression outputs a probability in $[0, 1]$.**\n",
    "\n",
    "\\begin{align*}\n",
    "h^{SVM}_\\theta(x) = \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            1 & z \\geq 0 \\\\\n",
    "            0 & \\textrm{otherwise}\n",
    "        \\end{array}\n",
    "    \\right.\n",
    "\\\\\n",
    "\\\\\n",
    "\\textrm{cost}_1(z) = max(0, k(z+1))\\\\\n",
    "\\textrm{cost}_0(z) = max(0, k(z-1))\\\\\n",
    "k=\\textrm{slope}\n",
    "\\\\\n",
    "\\\\\n",
    "\\textrm{for simplest \"linear kernel\" SVM} \\qquad z=\\theta^Tx \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $cost_0$ and $cost_1$ are the piecewise curves described above.\n",
    "\n",
    "\n",
    "<img src=\"images/pic17.png\" width=550></img>\n",
    "\n",
    "\n",
    "The full loss will be the sum of single point costs plus a regularization term:\n",
    "\\begin{align*}\n",
    "J_\\theta^{SVM} = C\\sum_{i=1}^m cost^{(i)} + \\frac{1}{2}\\sum^n_{j=1}\\theta_j^2,\n",
    "\\end{align*}\n",
    "\n",
    "By convention instead of $\\lambda$ weighting the regularization term we have $C$ weighting the data term, and we don't divide by the number of training points $m$. **If we set $C$ very large in the $J_\\theta$ expression, then minimizing $J$ will essentially be minimizing the regularization term subject to insisting that $\\theta^Tx$ > 1 when $y=1$ and vice versa for $y=0$ (to get 0 cost).** Recasting the dot product  $\\theta^Tx^{(i)}$ as $||\\theta|| \\cdot (\\textrm{projection x onto }\\theta) = p^{(i)}||\\theta||$, we can write this as:\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\textrm{minimize} \\; ||\\theta|| \\; \\textrm{s.t.} \\\\\n",
    "\\\\\n",
    "\\begin{array}{ll}\n",
    "            p^{(i)}||\\theta|| \\geq 1 \\qquad \\textrm{if } y^{(i)}=1 \\\\\n",
    "            p^{(i)}||\\theta|| \\leq -1 \\qquad \\textrm{if } y^{(i)}=0\n",
    "\\end{array}\n",
    "\\end{align*}\n",
    "\n",
    "Imagine our points $x$ live in $R^2$ (two features, $x_1$ and $x_2$) as in the picture below. We can interpret the dot product $\\theta^Tx$ as $||\\theta|| \\cdot (\\textrm{projection x onto }\\theta)$. To have $||\\theta|| \\cdot p^{(i)} \\gg 1$ while simultaneously pushing $||\\theta||$ as small as possible, we are forced to push $p^{(i)}$ as large as possible. This results in choosing the direction of $\\theta$ as pointing along the line between the two points of closest-approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/pic19.png\" width=550></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span> <span style=\"background-color:#FFFFbb\"> In SVM algorithms hypothesis takes the form of a binary 0 or 1 output depending on whether $z$ (equal to $\\theta^Tx$ in the simplest SVM) is greater or less than 0. The SVM cost function is similar shape to log loss, but is a simpler piecewise curve which is a flat line in the region where log loss is asymptotically zero, joining up to a straight line with a finite slope. The transition points are $z=1$ (for the y=1 curve) and $z=-1$ (for the y=0 curve). When you construct a full regularized loss function with this and put very heavy weighting on the data term, then the fitting amounts to minimizing the norm of $\\theta$ subject to the constraint that the dot product of $\\theta$ with $x$ is greater (less) than 1 when y is 1 (0). The result of this is that the minimizing $\\theta$ will point exactly along a line between the two clusters and the decision boundary, defined by $\\theta^T x = 0$, is perpendicular to that line. For this reason SVM will return decision boundaries that have the widest margin from the boundary to the two groups, and is called a \"Large Margin Classifier\".</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernels\n",
    "Above we constructed the simplest SVM, where $z=\\theta^Tx$. Recall that in logistic regression we can get a complex decision boundary shape in feature-space by including many higher order polynomial terms, but what if we made new features with a different kind of transformation? **A similarity transform of your input $x$ is a function that returns its similarity to some fixed \"landmark\" point, $l^{(i)}$, in feature space. The similarity transform can have lots of different functional forms which are called kernels.** Here is an example with a Gaussian kernel:\n",
    "\n",
    "\\begin{align*}\n",
    "\\textrm{new feature } \\qquad f_1 = \\textrm{similarity}(x, l^{(1)})\\\\\n",
    "\\textrm{for example,} \\qquad f_1 = exp(- \\frac{ ||x - l^{(1)}||^2}{2\\sigma^2}) = k_{gauss}(x, l^{(1)}).\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here $\\sigma$ is a parameter of the kernel function. Since norm is positive definite, the gaussian kernel will range from 1 (when $x \\approx l$) to 0 (when $x$ is very far from $l$). If the data is very widely spread out (large $\\sigma$) then the kernel output slowly and gently decays from 1 to 0, and vice versa for very clustered data, thus including $\\sigma$ lets the similarity transform automatically adjust it's sensitivity relative to how spread out the data is. \n",
    "\n",
    "<img src=\"images/pic20.png\" width=550></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can choose as many landmarks and thus construct as many of these new features as we like. The kernel maps our our input feaures, $x$, to our new transformed features, $f$. Our hypotheses will have the form:\n",
    "\n",
    "\\begin{align*}\n",
    "h^{SVM}_\\theta(f(x)) = \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            1 & \\theta^Tf \\geq 0 \\\\\n",
    "            0 & \\textrm{otherwise}\n",
    "        \\end{array}\n",
    "    \\right.\n",
    "\\\\\n",
    "z = \\theta^Tf = \\theta_0 + \\theta_1f_1 + ... \\theta_nf_n.\n",
    "\\end{align*}\n",
    "\n",
    "For any point close to the $j^{th}$ landmark, all values of $f_{i\\neq j}$ will be near zero, while $f_j \\approx 1$ and thus $z\\approx f_j$. For this reason the decision boundary will look like a region that encompasses the landmarks whose corresponding weight $\\theta_i$ is positive, tending to give $z>0$ and thus outputting the prediction $y=1$. \n",
    "\n",
    "### Choosing Landmarks\n",
    "Commonly we create a landmark exactly at every $x$ point in your training set. In this way our new features, $f$, which are the similarity transforms of $\\textrm{sim}(x^{(i)}, l^{(i)})$, will indicate how close a new point is to the points we already saw in training. The hypothesis and the cost function will now be functions of the new features $f$, and the parameters $\\theta$ are chosen, as always, by minimizing the loss. Finally, it is conventional to replace the $\\theta$-norm penalty term with a penalty $\\theta^T M \\theta$ for some matrix of $M$ that depends on the kernel - this gives a slightly different distance metric and lets the algorithm scale more effectively. To summarize:\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "f^{(i)} = \\textrm{sim}(x, l^{(i)})\\\\\n",
    "\\\\\n",
    "h^{SVM}_\\theta(x) = \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            1 & \\theta^Tf \\geq 0 \\\\\n",
    "            0 & \\textrm{otherwise}\n",
    "        \\end{array}\n",
    "    \\right.\n",
    "\\\\\n",
    "\\\\\n",
    "\\textrm{cost}_1(z) = max(0, k(z+1))\\\\\n",
    "\\textrm{cost}_0(z) = max(0, k(z-1))\\\\\n",
    "z=\\theta^Tf, \\qquad k=\\textrm{slope}\n",
    "\\\\\n",
    "\\\\\n",
    "J_\\theta^{SVM} = C\\sum_{i=1}^m \\bigg[y^icost_1(\\theta^Tf) + (1-y^i)cost_0(\\theta^Tf)\\bigg] + \\theta^T \\mathbf{M} \\theta,\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could certainly use kernels to generate new features for other algorithms, like logistic regression, but it will be computationally very expensive while the form of the penalized SVM allows for tricks that make this feasible.\n",
    "\n",
    "### SVM Meta Parameters\n",
    "We need to choose the inverse regularization strength, $C$, as well as the kernel parameter $\\sigma^2$. As expected, for large $C$ we increase variance, while for small $C$ we increase bias. For large $\\sigma$ the gaussian is less peaked, so the value of the features $f$ will vary slowly in space. Consequently a slightly different training point $x^{(i)}$, with a corresponding slightly different landmark $l^{(i)}$, will not result in a drastically different value of $f^{(i)}$ for a given test point. Thus large $\\sigma^2$ gives smaller variance, and vice versa. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## SVM's in Practice\n",
    "### Kernel Options\n",
    "The simplest SVM we started with, where there is no transformation and $z=\\theta^Tx$, is called the \"linear kernel\". This is appropriate when your number of input features $n$ (the length of $x$ vectors) is large, while the number of training points $m$ is small. The Gaussian kernel discussed above might be appropriate in the other case of small $n$, large $m$. It is very important to do feature scaling, especially for the gaussian kernel.\n",
    "\n",
    "Valid similarity transforms must satisfy Mercer's Theorem to play nicely with implementations of the loss minimization for SVMS. A less common option is Polynomial Kernel $k_{\\alpha, \\beta} = (x^Tl + \\alpha)^\\beta$, where $\\alpha$ and $\\beta$ are parameters of the kernel. \n",
    "\n",
    "### Multiclass Classification\n",
    "Just like with logistic regression, you can use one-vs-rest approach and train $K$ binary classifier SVMs for your $K$ labels. You will wind up with a fitted $\\theta$ vector for each binary classifier, and your final output should just be the label for the SVM that gives the largest $\\theta^Tf$ for an input $x$.\n",
    "\n",
    "### Logistic Regression vs. SVM\n",
    "- If you have $n > m$, say for a spam classifier with $n=10000$ words (features) but only $m =$ 10 to 1000 emails to train on, then logistic regression or SVM with linear kernel is likely to work best. You don't have enough data to accurately fit a complex model. Note that we \"derived\" linear kernel SVM with some minor modifications to logistic regression, so they are really quite similar algorithms and will perform similarly.\n",
    "\n",
    "- If you have a small number of features $n$ (1 to 1000) and an intermediate amount of training points $m$ (10 to 10000) then typically SVM with gaussian kernel will perform best. \n",
    "- If $n$ is small (1 to 1000) and $m$ is very large (50000 or more points), then SVM with a kernel might be computationally difficult so a good approach is to manually generate some new features and then use logistic regression or linear kernel SVM.\n",
    "\n",
    "Note a neural net would probably work well for all of the above cases, but it would be slower to train. SVM also has the advantage of being a convex optimization problem, so a global minimum will always be found.\n",
    "\n",
    "## Other Resources\n",
    "- http://www.cise.ufl.edu/class/cis4930sp11dtm/notes/intro_svm_new.pdf\n",
    "\n",
    "# Week7: SVM Discussion With David\n",
    "The idea of setting each training point to a landmark and using the Gaussian kernel for SVM struck David as very similar to Kernel Density Estimation. This is a technique for estimating the generating distribution of some random variable $X$ that you have $n$ observations of. The estimate of the density is formed by a mixture of Gaussians centered at all the observations and all weighted by $1/n$. For classification you can imagine making estimating a density for the $y=0$ points and likewise for the $y=1$ points, and then classifying new points based on which ever density gave the highest probability. Such a method doesn't take into account relative frequency of the two labels, but you could do so by taking a Bayesian with a prior taken from the relative frequencies and a likelihood taken from the density estimates. \n",
    "\n",
    "In SVM with a Gaussian Kernel, if you take every training point to be a landmark, then the quantity $\\theta^Tf$ is a weighted mixture of Gaussians. In contrast to classification with the kernel density estimates, the Gaussians resulting from the $y=1$ and $y=0$ points are combined into the mixture but their coefficients will have opposite sign. Consequently the sign of $\\theta^Tf$ which dictates the binary hypothesis output serves as a comparator between the \"density estimate\" of the $y=1$ points, and the \"density estimate\" of the $y=0$ points. However here there is another difference that the Gaussian weightings are not all equal at $1/n$ but rather are the fitted values $\\theta$.\n",
    "\n",
    "Some remaining questions: what are the conditions under which SVM would look like normal kernel density estimator classification i.e. fitted $\\theta$ values reflecting $1/n$ of each subpopulation? How is SVM better than a Bayesian approach to classifying with kernel density estimators - what does fitting the weightings buy you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span> <span style=\"background-color:#FFFFbb\"> In SVM algorithms we use $z = \\theta^Tf$ where $f$ is a vector of new features that are similarity transforms of our input $x$. A similarity transform is a function that takes in the distance between $x$ and some fixed \"landmark\" point, $l^{(i)}$, in feature space, and the different functional forms are called kernels. The most common kernel is the Guassian $k_{gauss}(x, l^{(1)}) = exp(- \\frac{ ||x - l^{(1)}||^2}{2\\sigma^2})$, while no transformation at all ($z=\\theta^Tx$) is called the Linear Kernel. Commonly a landmark is placed exactly at every $x$ point in your training set: in this way $f$ will indicate how close a new point is to the points we already saw in training. The relative sizes of number of features $n$ and number of training points $m$ dictates when to use logistic regression and linear kernel SVM versus using Gaussian Kernel SVM.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week8: Clustering\n",
    "Clustering is an unsupervised algorithm, meaning it only needs input $X$ with no labels $y$, that identifies coherent subsets, or \"clusters\", of data points within the feature space. Even if your data is not well separated, sometimes clustering can be helpful. If you are a t-shirt manufacturer and you would like to know how to size your S, M and L shirts, you could run $k=3$ clustering on (height, weight) data even though these points are not expected to cluster strongly.\n",
    "\n",
    "## K-Means Clustering\n",
    "This is the most common clustering algorithm. For a fixed $K$, it proceeds as follows:\n",
    "- Randomly initialize $K$ points in feature space, called the *cluster centroids*\n",
    "- Repeat to minimize objective function:\n",
    "    1. Assign each data point to the cluster whose centroid it is closest to.\n",
    "    2. Recompute the location of each cluster centroid as the average position of points in that cluster.\n",
    "\n",
    "**In this case convergence means the *cluster centroids* locations don't really change with further iterations.** If on any iteration there is a cluster which has no points assigned to it, typically that cluster is dropped (giving $K-1$ clusters).\n",
    "\n",
    "The terminology is as follows:\n",
    "- vector location of the $i^{th}$ cluster centroid is $\\{\\mu_i\\}$\n",
    "- assignment of the $i^{th}$ data point to the $j^{th}$ cluster is denoted $c^{(i)} = j$\n",
    "- vector location of the centroid for the cluster to which the $i^{th}$ point is assigned is $\\mu_{c^{(i)}}$.\n",
    "\n",
    "The objective function that is minimized by such an algorithm is sometimes called the *Distortion Cost Function*. It minimizes the sum of squared distances between each point and its cluster's centroid:\n",
    "\\begin{align*}\n",
    "J(c^{(1)},...,c^{(m)}, \\mu_1,...,\\mu_K) = \\sum_{i=1}^m ||x^{(i)}-\\mu_{c^{(i)}} ||^2\n",
    "\\end{align*}\n",
    "\n",
    "In the text above, step #1 of the algorithm minimizes $J$ while keeping cluser centroid locations fixed, while step #2 minimizes $J$ while keeping the data points assignments to clusters fixed.\n",
    "\n",
    "**A good method for the random initialization at the beginning of the algorithm is to pick $K$ of your data points at random and use those as the first centroid locations.** This algorithm can converge to different final clusters depending on initialization because there can be many local optima of $J$. For reasonably small $K$, say 10 clusters, running the algorithm around 50 to 1000 times and picking the best result is a good approach.\n",
    "\n",
    "### Choosing Number of Clusters\n",
    "In many data sets it is hard to visualize the data and even with visualization sometimes the number of clusters is really ambiguous. A common way to choose $K$ is called the **elbow method**. We will always get a smaller $J$ by adding an additional cluster, but the rate at which $J$ decreases with $K$ will be initially large, but later it will be small once $K$ is larger than the true number of clusters (since we aren't gaining as much by finer splitting of clusters). For some data sets this change in behavior results in a sharp kink or \"elbow\" in the $J$ versus $K$ plot. \n",
    "\n",
    "**If you are using K-means as an intermediate step in generating features, then you can optimize over $K$ by looking at the performance of the full procedure.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span> <span style=\"background-color:#FFFFbb\"> K-Means is an unsupervised algorithm that groups unlabeled data into $K$ coherent clusters, each cluster has a *centroid* located at the average position of it's constituent points. Objective function to minimize is the sum of squared distances between points and their assigned cluster centroids. An algorithm which minimizes this is (1) randomly initialize $K$ centroid locations, then (2) repeatedly assign points to their nearest centroid and then recompute centroid locations. This procedure has local optima so it is best to randomly initialize and run the algorithm 50 - 1000 times. The test of whether you have chosen a good *K* is the downstream performance of your full algorithm, but an \"elbow\" in the $J$ vs. $K$ can also be indicative.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8: Dimensionality Reduction\n",
    "Dimensionality reduction is another unsupervised algorithm that operates just on $X$. The main motivations are speeding up computations by data compression, and data visualization since we struggle to visualize anything above $R^3$.\n",
    "\n",
    "Imagine feature vectors in $R^2$ in which $x_1$ and $x_2$ are highly correlated: the data points will lie on a somewhat straight line. In this case we could project each point down onto that straight line to get a single feature, $z$, which is the distance between the origin and the projection. This new feature preserves most of the information from $R^2$ in that points which were close together in the plane will be close together in $z$ value, and vice versa for points that were far apart. This wouldn't be the case if our data didn't follow an approximately straight line - in that case we *would* lose information by the compression.\n",
    "\n",
    "Likewise for projecting data in $R^3$ down onto a plane and compressing each point's representation to just two numbers. Sometimes you may even find that the new constructed features (dimensions) have some useful interpretation. \n",
    "\n",
    "<img src=\"images/pic21.png\" width=550></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principle Component Analysis (PCA)\n",
    "PCA is by far the most commmon algorithm for dimensionality reduction. **PCA operates by trying to find a subspace (line for $R^2$, plane for $R^3$, etc.) that minimizes the Projection Error: the sum of squared distances from each point to it's projection into the chosen subspace (line, plane etc.).** If you can find a subspace where the projection error is small, that means the data do not have much \"spread\" along the direction perpendicular to it and thus it is a direction that can be safely discarded.  \n",
    "\n",
    "\\begin{align*}\n",
    "\\textrm{Average Projection Error:} \\qquad \\frac{1}{m}\\sum_{i=1}^m ||x^{(i)} - x^{(i)}_{approx}||^2,\n",
    "\\end{align*}\n",
    "\n",
    "where $x^{(i)}$ is an original point and $x^{(i)}$ is the location of it's projection into the subspace.\n",
    "\n",
    "In the squared error metric of linear regression we care about the distance between the real and predicted values of a particular *component* of the data point - namely the $y$ component. In PCA there are no labels, all dimensions $x_1,...,x_n$ are treated equally, and so it is the *perpendicular* distance from a point to the chosen subspace that matters. PCA is *not* linear regression.\n",
    "<img src=\"images/pic22.png\" width=550></img>\n",
    "\n",
    "### Feature Scaling\n",
    "It is critical to perform feature scaling prior to PCA, namely:\n",
    "\\begin{align*}\n",
    "x_i \\rightarrow \\frac{x_i - \\mu_i}{\\delta_i},\n",
    "\\end{align*}\n",
    "where $\\mu_i$ is the mean value of the $i^{th}$ feature, and $\\delta_i$ is some measure of the spread of values of that feature, like variance or [max - min]. Without normalization, the \"spread\" of the data in a particular direction would be sensitive to the scale of that observable (or linear combination of observables).\n",
    "\n",
    "\n",
    "### PCA Algorithm\n",
    "In general to reduce $n$-dimensional data to $k$ dimensions you need to find $k$ linearly independent \"directions\" in the space $R^n$ which will define (span) the subspace you are projecting into. We have an analytical algorithm for choosing these directions which can be proven to minimize the Projection Error:\n",
    "1. Compute Covariance Matrix $\\Sigma$ (the $ij^{th}$ element describes how much the $i^{th}$ and $j^{th}$ components of the random vector change together.)\n",
    "2. Use SV decomposition to get the eigenvectors of $\\Sigma$.\n",
    "3. Take the submatrix which is the first $k$ columns of the SV's *U-Matrix*, call this $U_{reduce}$.\n",
    "4. $U_{reduce}^T$ is $k$x$n$ sized and maps data points to their new reduced-dimensionality representation: $z=U_{reduce}^Tx$.\n",
    "\n",
    "The representation of the projections in the original space can be had by $x_{\\textrm{approx}} = U_{\\textrm{reduce}}z$. These will be reasonable approximations to the original data points, as long as the projection error was small.\n",
    "\n",
    "\\begin{align*}\n",
    "\\textrm{compress:} \\qquad z=U_{reduce}^Tx\\\\\n",
    "\\textrm{decompress:} \\qquad x_{\\textrm{approx}} = U_{reduce}z\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing $k$\n",
    "Certainly as we decrease $k$ our objective function grows (starting from $k=n$ it will be 0), so we want $k$ as small as possible without sacrificing significant *information* from the original data. **When you reduce to a subspace you want the spread of the data along the direction orthogonal to that space to be very small - that's what justifies discarding the information contained by that orthogonal direction.** This means after the reduction you won't have lost much of the total variance. When you go from $k$ to $k-1$ dimensions, how much of the \"variance\" or \"spread\" of the data is still retained?\n",
    "\n",
    "**Since we always center our data, $\\mathrm{E}[x] = \\vec{0}$ and so $1/m \\sum^m||x^{(i)}||^2$ is a measure of random vector total variance. A common criteria for choosing $k$ is to make it as small as possible while losing no more than 1% of the original data set variance.** With our definition of random vector variance, the lost variance is just the projection error so we can write the condition as:\n",
    "\n",
    "\\begin{align*}\n",
    "\\textrm{Choose $k$ as large as possible s.t.:} \\qquad \\frac{\\frac{1}{m}\\sum_{i=1}^m ||x^{(i)} - x^{(i)}_{approx}||^2}{\\frac{1}{m}\\sum_{i=1}^m ||x^{(i)}||^2,} \\leq 0.01\n",
    "\\end{align*}\n",
    "\n",
    "In the SV decomposition of the PCA algorithm the diagonal elements of the $\\mathbf{S}$ matrix can tell us the fraction of variance lost for a given $k$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\textrm{Fraction of Var Retained at k:} \\qquad \\frac{\\sum_{i=1}^k S_{ii}}{\\sum_{i=1}^n S_{ii}}.\n",
    "\\end{align*}\n",
    "\n",
    "For visualization purposes $k$ is usually chosen to be 2 or 3 for obvious reasons.\n",
    "\n",
    "### Tips for Applying PCA\n",
    "**You should use PCA only for compression (or visualization) purposes, not to prevent overfitting. Dimensionality reduction necessarily discards information and it does so \"blindly\" without reference to how valuable that information is in predicting $y$ values.** You can imagine PCA discarding a dimension in which the data is not very spread out, even if it turns out that $y$ is extremely sensitive to even small differences in that dimension. Regularization can achieve the same result of preventing overfitting, and it does so in a way that is sensitive to how useful each dimension is. \n",
    "\n",
    "You should only use training data, not test data, to \"fit\" $U_{reduce}$ to choose a $k$: think of this as a meta-parameter of your algorithm that is subject to overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More on PCA from David\n",
    "Your n-dimensional data set starts out expressed in the basis of the measured variables (observables), but imagine doing a change of basis to express the same data points in a new basis of *composite variables*. Each data point now has a component in each of the composite directions. **For some composite directions, the data points may all have similar components, this means the composite variable corresponding to that direction can't have much explanatory power in predicting the response variable (if you're doing function estimation with your data). If you're doing density estimation you might say that the composite variable doesn't have any significant randomness to it. One very common reason this might happen is if some of your observables are correlated. **\n",
    "\n",
    "A danger in applying PCA to your data set is that because you are looking through all the possible sets of composite variables (bases) you might expect that there is a reasonable chance that you will find a reduced basis where the response variable values looks pretty well separated, even if the true underlying function or population doesn't reflect this. One trick you can use to avoid this trap is to take your data set and swap all the response variable values in a randomized way, and then repeat the PCA and visualization. If you do this a few times and then look at the panel of visualizations you should be able to clearly pick out the one from the real data set as distinguishing the response variable values much more than the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span> <span style=\"background-color:#FFFFbb\"> PCA is the most common algorithm for reducing dimensionality of data for compression (speed) or visualization purposes. We find a subspace of our original feature space (line for $R^2$, plane for $R^3$, etc.) that minimizes the *Projection Error*: the sum of squared distances from each point to it's projection into the subspace. If the projection error is small the data do not have much \"spread\" along the direction perpendicular to the subspace and thus that direction can be discarded without losing much predictive information. When compressing from $n$ to $k$ dimensions an analyical algorithm exists for finding the subspace that minimizes this error, but note it is CRITICAL to mean-normalize and scale your features first. When compressing for speed purposes, we choose $k$ to be as small as possible while still retaining 99% of the variance of the data, while for visualization purposes we use $k=2$ or 3. To prevent overfitting use regularization, not PCA, as PCA is blind to the labels and could discard disproportionately useful dimensions.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Week 9: Anomaly Detection\n",
    "Anomaly detection takes an unlabeled data set of $\\{x^{(i)}\\}$ points, fits a generating distribution, $p(x)$, for the data, and then uses this to decide if a new data point $x$ is an *anomaly*. For some threshold $\\epsilon$ we say that $p(x_{new}) < \\epsilon$ implies that $x$ is an anomalous point. This is useful in detecting fraud where the features are user activities or characteristics like typing speed. Other examples are quality control testing in manufactring or monitoring individual computers in a data center to see which ones might be having trouble. \n",
    "\n",
    "### Fitting p(x)\n",
    "The Gaussian distribution is used heavily in modeling $p(x)$ for anomaly detection. Recall that for data set $\\{x^{(i)}\\}$, if we model $x\\sim N(\\mu, \\sigma^2)$ then the MLE's for these parameters are simply the mean and variance of the data set. *[That is, in the family of normal distributions, the particular distribution where the parameters take these values is guaranteed to be the one which has the highest likehood (probability) of generating the observed data.]*\n",
    "\n",
    "For finding $p(x)$ we do the following:\n",
    "\\begin{align*}\n",
    "\\textrm{assume independence:} \\qquad p(x) = \\Pi p(x_i)\\\\\n",
    "\\textrm{assume each feature is normally distributed:} \\qquad x_i \\sim \\textrm{N}(\\mu_i, \\sigma^2_i)\\\\\n",
    "\\textrm{fit each feature distribution with the MLEs:} \\qquad \\mu_j = \\frac{1}{m}\\sum^m x_j^{(i)}, \\;\\; \\sigma^2_j = \\frac{1}{m}\\sum^m\\big(x_j^{(i)}-\\mu_j\\big)^2.\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice even if the features aren't quite independent this model usually works fine. \n",
    "\n",
    "### Finding the full algorithm ($\\epsilon$)\n",
    "Typically we are starting from a historical data set where it is known which points were actually anomalous (fraud, bad part, etc.) and which were normal. We use $y=1$ to denoate an anomaly. To build the full detection algorithm we do the following:\n",
    "\n",
    "1. Split data into *train*, *cv* and *test* sets, but where the training set contains only non-anomalous examples.\n",
    "2. Use the *train* set to fit p(x) as described above\n",
    "3. Use the *cv* set to choose $\\epsilon$ so that the prediction of $y$ gives the best score according to your choice of metric.\n",
    "4. Evaluate your final algorithm with the *test* set.\n",
    "\n",
    "**Note that you need to use an evaluation metric that is appropriate to skewed classes, false positives or negatives, precision or recall, or F1 score.**\n",
    "\n",
    "### When to use Anomaly Detection vs. Supervised Learning.\n",
    "Use anomaly detection when you have a very small number of positive (anomalous) examples, 0 to 20 is common, and a much larger number of negative examples. \n",
    "\n",
    "**In general, to use a supervised algorithm you want to feel that there are enough positive examples for the algorithm to get a sense of what a positive point really looks like, and to be confident that future $y=1$ points will look like the ones from your existing data.** In contrast, If there are many different types of anomalies, such as many different ways in which an aircraft engine from a production line can \"fail\", then a supervised learning algorithm may have a hard time learning an appropriate decision boundary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing Features\n",
    "- Consider replacing a feature with a transformation (log, powers) of that feature which appears more gaussian.\n",
    "- Try to define features that might take unusually large or unusually small values in anomalous events.\n",
    "- Use an iterative **error analysis** procedure where you inspect anomalous points where your algorithm fails, and try to think of new features that would help distinguish those specific points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Multivariate Gaussian Distribution\n",
    "The assumption that $p(x) = \\Pi p(x_j)$ means that $p$ can't adjust it's shape to represent off-axis directionality (feature *correlations*), as in the picture below (pink shows the countours of the fitted model):\n",
    "\n",
    "<img src=\"images/pic24.png\" width=550></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more complex model is the multivariate gaussian, which is parameterized by $\\mu \\in R^n$ and a covariance matrix $\\Sigma \\in R^{nxn}$. This model can capture *correlations* between the data such as $x_1$ tends to be higher than it's mean when $x_2$ is higher than it's mean, so it can stretch along any $x_i = x_2$ line. There is a similar analytical MLE for $\\Sigma$.\n",
    "\n",
    "Note that instead of switching to the multivariate you can use the simpler original model but create a new feature that will represent the correlation where $x_1$ and $x_2$ together are unusual in some sense. **The independent $x_j$ model is compuationally cheaper and can be fit with fewer data points because of it's smaller number of parameters.** A good rule of thumb is to insist $m > 10n$ if you want to use the multivariate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span> <span style=\"background-color:#FFFFbb\"> Anomaly detection fits a generating distribution to a set of \"normal\" points and then determines if new points are normal or anomalous based on some probability density threshold $\\epsilon$. The model is usually $p(x) = \\Pi p(x_j)$ where each $x_j$ is independently normal. Alternatvely a multivariate gaussian can be used to capture correlations among features. Assuming you have a few examples of anomalous points, you can choose $\\epsilon$ as the threshold that gives the best precision, recall, F1 score or other skewed-class metric. Feature choice makes a big difference: consider transforming features to make them more Gaussian, or doing error analysis to brainstorm features that would be especially large or small in the event of an anomaly. If you feel that there are enough positive examples (anomalies) for the algorithm to get a sense of what a positive point really looks like, and to be confident that future y=1 points will look like the ones from your existing data, then you may use a conventional supervised algorithm rather than anomaly detection. \n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 9: Recommender Systems\n",
    "Recommender systems like Amazon and Netflix use are one of the highest-rated problems by Silicon Valley companies.\n",
    "\n",
    "A prototypical system is a data set which indicates the 0-to-5 star rating given to a set of movies, by a set of users (some of these will be missing if a user hasn't rated a specific movie). We define:\n",
    "- $n_u$ the number of users, $n_m$ the number of movies\n",
    "- $r(i, j)$is an indicator for if the $j^{th}$ user has rated the $i^{th}$ movie\n",
    "- $y(i, j)$ is our prediction for the rating given by the $j^{th}$ user to the $i^{th}$ movie\n",
    "\n",
    "**The task is to make predictions $y$ for the missing ratings (where $r=0$).** \n",
    "\n",
    "### Content Based Recommendations\n",
    "This is a simple approach that relies on the *content* of a movie to predict a user's response. We begin with a feature vector $x^{(i)} \\in R^n$ for each movie, like $x_1$ being the amount of \"romance\" and $x_2$ being the amount of \"action\". Then we learn a set of parameters *specific to each user* $\\theta^{(j)} \\in R^{n+1}$ for a linear regression model of each user's predictions: $y(i, j) = (\\theta^{(j)})^Tx^{(i)}$. It's common to use regularized least-squares error in fitting $\\theta^{(j)}$ for each user. \n",
    "\n",
    "<img src=\"images/pic25.png\" width=550></img>\n",
    "\n",
    "###  Collaborative Filtering (\"Feature Learning\")\n",
    "Suppose we don't have any feature values for our movies, but instead we have features for each user, such as self-reporting how much they like romance movies in general, how much they like action movies in general, etc. **We can think of this as being *told* the values of $\\theta^{(j)}$ for each user and then *learning the values of features* $x^{(i)}$ for each movie.** This proceeds via regularized least-squares error fitting like above, where the roles of $\\theta$ and $x$ are just reversed! \n",
    "\n",
    "<img src=\"images/pic26.png\" width=550></img>\n",
    "\n",
    "You can actually converge to a reasonable result by first guessing some initial values of $\\theta$ for each user, and then cycling between learning new $x$ and learning new $\\theta$ values! In some sense users are collaborating, by giving their ratings, to help the system learn good features that will let it to make better predictions. **In practice we learn the $x$ and $\\theta$'s together by minimizing a total regularized cost function:**\n",
    "\n",
    "\\begin{align*}\n",
    "J(x^{(1)},..., x^{(n_M)}, \\theta^{(1)}, ..., \\theta^{(n_u)}) = \\textrm{all squared errors} \\; + \\; \\textrm{penalty on all $x_k$} \\; + \\; \\textrm{penalty on all $\\theta_k$},\n",
    "\\end{align*}\n",
    "\n",
    "which can be optimized by random initialization followed by gradient descent with the proper update rule. Note that with this approach we don't bother with the \"intercept\" feature $x_0 = 1$ and its corresponding $\\theta_0$\\ - if such a \"constant\" feature is actually useful then it will be learned during the optimization. \n",
    "\n",
    "After learning the full model you can predict missing ratings and use those to recommend movies you think a user might like. Another option is to **use the learned movie featues to find movies that are *similar* to a movie that a user just watched**, where we define similarity as smallness of $||x^{(i)}-x^{(j)}||$.\n",
    "\n",
    "### Low Rank Matrix Factorization\n",
    "The collaborative filtering algorithm can be vectorized to facilitate implementation as simply $\\mathbf{Y}_{predict}= \\mathbf{X}\\mathbf{\\Theta}^T$, where $\\mathbf{X}$ is a matrix where each row is a movie (feature values), and $\\mathbf{\\Theta}$ is a matrix where each row is a user (parameter values). This gives $Y_{ij} = y(i,j) = (\\theta^{(j)})^Tx^{(i)}$. This matricized form of the collaborative filtering model is a *Low Rank Factorization* of $\\mathbf{Y}$ (just like you would factor $21=3x7$). It turns out that a lot of algorithms in ML can be thought of a trying to factor a matrix. Below I'm reproducing some amazing slides from a [PyData talk by Chris Thurau](http://www.slideshare.net/PyData/thurau-pydata-2014)\n",
    "\n",
    "<img src=\"images/pic_matrix_factorization.png\" width=800></img>\n",
    "\n",
    "In fact *Factorization Machines* are thrown out as a new ML algorithm. See these two technical papers: [1](https://www.ismll.uni-hildesheim.de/pub/pdfs/Rendle2010FM.pdf) and [2](https://www.ismll.uni-hildesheim.de/pub/pdfs/Rendle2010FM.pdf).\n",
    "\n",
    "### Mean Normalization\n",
    "Consider a user who hasn't rated any movies yet. In the combined cost function there will be no contribution to the squared error for this user, and so the regularization term will be free to pull all her $\\theta$ parameters to zero, which is not going to let us make any useful predictions.\n",
    "\n",
    "**Mean normalization centers each movie's ratings by subtracting the average rating for that movie, and then inverts this adding back in the average rating when making predictions: $y(i,j) = (\\theta^{(j)})^Tx^{(i)} + \\mu_i$**. That way for a new user with $\\theta = \\vec{0}$ we will just predict the average rating for each movie. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span> <span style=\"background-color:#FFFFbb\"> Recommender systems are very useful algorithms for systems where you have space of \"users\" and a space of \"products\", your data are \"ratings\" that specific users have given to specific products, and you want to be able to recommend more products to a user. A content-based approach takes known features for each product that reflect it's characteristics, $x$, and models a users rating as $y=\\theta x$, where $\\theta$ is a set of parameters specific to that user. The $\\theta$ for each user are fitted by regularized squared error cost, and then they can be used to predict missing ratings for products.  Collaborative filtering starts from only the existing ratings and learns the set of both produc features $x$ and user parameters $\\theta$, again minimizing a regularized squared error cost. The learned features can also be used to find movies that are \"similar\" to eachother in that feature space. Mean normalization centers each products's ratings by subtracting the average rating for that product, and then inverts this adding back in the average rating when making predictions. This prevents new users who have never rated anything from predicting zero on all products, since their parameters will be learned as $\\theta = \\vec{0}$ due to regularization.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 10: Gradient Descent with Big Data\n",
    "Good performance often comes from a high-complexity model (low-bias) with large amounts of data to train on. In many cases the actual model choice is usually less important than the amount of data you have to train on. Having 100 million observations is pretty common for modern data sets. \n",
    "\n",
    "## Batch vs. Stochastic vs. Mini-Batch\n",
    "In **Batch Gradient Descent**, which we learned originally, for every time that you update any single parameter you need to first calculate the exact value of the partial derivative of $J$. Recall we had, for simple linear or logistic regression:\n",
    "\n",
    "> **Batch Gradient Descent with Squared Error**: $ \\theta_j := \\theta_j - \\alpha\\frac{1}{m}\\sum^m_{i=1}\\big(h(x^{(i)})-y^{(i)}\\big)x^{(i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That means to make a single update to a single parameter involves summing over your 100 million observations. This is slow, and won't if your whole data set doesn't fit in memory at once. A good alternative is **Stochastic Gradient Descent**. It instead looks at one single observation at a time and updates all the parameters to fit that observation a little better. The full specification is\n",
    "- Shuffle your data set randomly\n",
    "- For each observation in the training set $i=1,... m$:\n",
    "    - For each parameter $j=1,... n$:\n",
    "        - update $\\theta_j := \\theta_j - \\gamma e_{i}x^{(j)}$, where $e_{i} = \\big(h_\\theta(x^{(i)}) - y^{(i)}\\big)$ is the error on this specific observation.\n",
    "\n",
    "SGD converges along a more random path and noisy path, and never really hits the minimum, but rather just continually wanders around in the neighborhood of it. To suppress this behavior you can use an adaptive learning rate that decreases over the course of the algorithm: $\\gamma = \\frac{\\alpha}{\\textrm{(number iterations)}+\\beta}$, where $\\alpha$ and $\\beta$ are just constants. For very large data sets sometimes just a single run through the data with SGD is sufficient for convergence. You can monitor convergence by plotting a moving (windowed) average of say 1000 single-observation costs, $cost_{ij} = h_\\theta(x^{(i)})-y^{(i)}$, as you loop through the points. \n",
    "\n",
    "Mini-batch is yet another variation that partitions the observations into a few different smaller batches and then executes like Batch GD on each of those subsets in turn.\n",
    "\n",
    "- Batch: LOOP{ For each parameter, update it to give a better overall fit to all the observations. }\n",
    "- Stochastic: LOOP{ For each single observation, update all the parameters to fit that single observation better. }\n",
    "- Mini-Batch: LOOP { For each parameter, update it to give a better overall fit to a *SUBSET* of all observations }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span> <span style=\"background-color:#FFFFbb\"> In Batch Gradient Descent, for each update on each parameter you need to calculate the full value of the partial of $J$, which means summing over all the training examples. This is infeasible for large or out-of-memory sets. An alternative is Stochastic Gradient Descent which loops through the training points one at a time and for each point it updates all the parameters to fit that single point a little better. SGD will converge in a more circuitous way and continue to wander around in the neighborhood of the minimum, although you can ameliorate this by using a slowly decreasing learning rate. Mini-batch is yet another variation that partitions the observations into a few different smaller batches and then executes like Batch GD on each of those subsets in turn.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 10: Advanced Topics\n",
    "\n",
    "## Online Learning\n",
    "There are many examples in e-commerce now of websites or web apps that are privy to a continuous stream of data from their users. **Online learning refers to letting a ML algorithm continually learn as it receives each new observation from the stream, like a stochastic gradient descent that never ends!** This lets the learned parameters change over time if the underlying behavior of the system is changing, and also allows you to use and then immediately discard data rather than having to warehouse it. Predicting click through rate on high traffic websites is a very common example of such a system. Recommender systems are also often run in this fashion.\n",
    "\n",
    "A good example is a shipping service where visitor to the site inputs their desired origin and destination, is shown a price, and either purchases or doesn't purchase the service. We'd like a model that gives the probability that the purchase will be made based on the three features (origin, destination and price) so we can decide how to set prices. This could be set up as a simple logistic regression model on the server, which learns it's parameters $\\theta$ by SGD using the stream of observations being generated in real time from the site. \n",
    "\n",
    "## Map Reduce and Data Parallellism\n",
    "**SGD is a way of dealing with the fact that batch gradient descent requires a sum over a crap ton of observations. MapReduce is an alternative way of dealing with this that relies on farming out pieces of the sum to different computers.** The key to using a MapReduce approach is whether your learning algorithm can be expressed as a sum over the training set. If so, each individual server can own a subset of the training set, compute the relevant sum over that subset, and send it to a central server which can just sum up all the responses from the servers.\n",
    "\n",
    "<img src=\"images/pic27.png\" width=550></img>\n",
    "\n",
    "MapReduce also applies to parallelizing a computation over multiple cores (CPUs) on a single machine. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span> <span style=\"background-color:#FFFFbb\"> Many websites or web apps  are privy to a continuous stream of data from their users. Online learning refers to letting a ML algorithm continually learn as it receives each new observation from the stream, like a stochastic gradient descent that never ends! This lets the learned parameters change over time if the underlying behavior of the system is changing, and also allows you to use and then immediately discard data rather than having to warehouse it. SGD is a way of dealing with the fact that batch gradient descent requires a sum over a crap ton of observations. MapReduce is an alternative way of dealing with this that relies on farming out pieces of the sum to different computers. The key to using a MapReduce approach is whether your learning algorithm can be expressed as a sum over the training set. If so, each individual server can own a subset of the training set, compute the relevant sum over that subset, and send it to a central server which can just sum up all the responses from the servers.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 11: Application Example - Photo OCR\n",
    "The photo OCR problem is how to get computers to recognize text in images. This has been well solved for scanned text documents but is still difficult with actual pictures. \n",
    "\n",
    "## Pipelines\n",
    "A typical approach to this problem would be:\n",
    "- identify regions of text in the image\n",
    "- segment each region into individual character images\n",
    "- run a classifier on each individual character image\n",
    "- (optionally spell check the resulting words)\n",
    "This is an example of a **Pipeline** where the full problem is broken into modules which feed one another sequentially. \n",
    "\n",
    "<img src=\"images/pic28.png\" width=550></img>\n",
    "\n",
    "### Step 1: Text Detection\n",
    "For the first step of text detection we can use a sliding window approach. For instance, to find pedestrians in images you decide on a rectangle aspect ratio typical to a pedestrian, you find a bunch of positive (pedestrian) and negative (non-pedestrian) images cropped to those dimensions, and you train a classifier. Then you take your original image and slide a rectanglar window around to different locations (according to a \"step-size\" parameter) on the full image, running your classifier at each location. You would need to do this with different rectangle sizes to capture pedestrians at different distances from the camera (and each rectange then need to be rescaled to match the pixel dimensions that you trained your classifier on). For text detection you do a similar procedure, but then you need to use an \"expansion operator\" that will combine neighboring regions identified as text (since they are probably part of the same word or sentence). Finally, since we expect regions of text to have a certain aspect ratio we can discard e.g. circular regions.\n",
    "\n",
    "<img src=\"images/pic29.png\" width=550></img>\n",
    "\n",
    "\n",
    "### Step 2: Character Segmentation\n",
    "For the second step we can take each identified text region and use a uni-directional sliding window technique to pan across it in steps, running a classifier on the sub-image at each step. In this case our classifier should be trained on example images of text where there is a space separating two characters on either side of the image ($y=1$) i.e. we are trying to find the spaces.\n",
    "\n",
    "## Artificial Data Synthesis\n",
    "This comes in two flavors, creating new data from scratch versus amplifying an existing small set of data. For the character classification example you can grab images of one or a few characters from vast font libraries and paste them againt random backgrounds (usually with some added blurring or other distortion). Alternatively you can take each existing image in your set and apply different random distortions to it to produce new samples! **The key to synthesizing *useful* new data is that that the new elements should be representative of what you expect to see in the test set. Ideally the noise and distortion used should have a physically realistic basis - it usually doesn't help to add completely random noise.**\n",
    "\n",
    "As always, make sure you have a low-bias classifier (add new features as needed) before bother to get more data. Note that sometimes manually gather and labeling more data yourself takes less time than you might think.\n",
    "\n",
    "## Ceiling Analysis\n",
    "Which component in the pipeline should you allocate your scarce resources to? **First you need a single real number evaluation metric for result of the full pipeline.** In this example we could use fraction of characters in a test set of images that we classify correctly. Run your test set images through your actual pipeline to get a value for this, say 72%. Now you go in and manually do the job of the first component in your pipeline e.g. you manually draw rectanges around the text in the test set images. How much does the final metric improve when you do this step perfectly by hand? Now repeat this but also manually do the second stage (character segmentation) with perfect accuracy. How much does this improve the final metric? \n",
    "\n",
    "<img src=\"images/pic30.png\" width=550></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color:#d3a2f0\">**Summary**:</span> <span style=\"background-color:#FFFFbb\">Many systems can be represented by a **Pipeline** where the full problem is broken into modules which feed one another sequentially. A **ceiling analysis** is helpful to indicate which component(s) have the largest upside on the final evaluation metric. You begin with some reasonably small test set of inputs, and you run it through the full pipeline to get a baseline score. Next you manually provide perfect \"answers\" for the first component of the pipeline and see how much the final score improves, then add in perfect answers for the second component and see again the improvement. Large jumps in improvement indicate a component where there is a large upside to your final metric if you can make improvements. Artificial data synthesis can help train high variance models by creating new data from scratch or amplifying an existing set by applying variations to each member (like blurring, distortion or rotation images). The key to synthesizing *useful* new data is that that the new elements should be representative of what you expect to see in the test set. Ideally the noise and distortion used should have a physically realistic basis - it usually doesn't help to add completely random noise.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 11: Conclusions   :(\n",
    "\n",
    "<img src=\"images/pic31.png\" width=550></img>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
