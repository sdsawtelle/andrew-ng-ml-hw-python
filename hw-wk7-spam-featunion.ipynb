{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# In which I help you decide if cash4u@freestuff.net is legit or not.\n",
    "\n",
    "Week 7 of Andrew Ng's ML course on Coursera introduces the Support Vector Machine algorithm and challenges us to use it for classifying email as spam or ham. Here I use the [SpamAssassin public corpus](https://spamassassin.apache.org/publiccorpus/) to build an SVM spam email classifier in order to learn about the relevant python tools. [Part I](http://sdsawtelle.github.io/blog/output/spam-classification-text-processing.html) focused on the preprocessing of individual emails, but now I'm going to actually do some machine learning. \n",
    "\n",
    ">## Tools Covered:\n",
    "- `CountVectorizer` for mapping text data to numeric word occurrence vectors\n",
    "- `tfidfTransformer` for normalizing word occurrence vectors \n",
    "- `Pipeline` for chaining together transformer (preprocessing, feature extraction) and estimator steps\n",
    "- `GridSearchCV` for optimizing over the metaparameters of an estimator or pipeline\n",
    "- `FunctionTransformer` for creating custom transformer objects from a function\n",
    "- `FeatureUnion` for combining features from different feaure-creation pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sonya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Set up environment\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "import snips as snp  # my snippets\n",
    "snp.prettyplot(matplotlib)  # my aesthetic preferences for plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sonya\\Box Sync\\Projects\\course-machine-learning\\hw-wk7-spam-svm\n"
     ]
    }
   ],
   "source": [
    "cd hw-wk7-spam-svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Incorporating Other Features in the Pipeline with `FeatureUnion`\n",
    "We might do even better by incorporating the other numeric features that we created during the email preprocessing in [Part I: NLP](http://sdsawtelle.github.io/blog/output/spam-classification-text-processing.html). Remember these were things like the number of HTML tags and the number of blank lines. We still would like to utilize a grid search, but it's not clear how these other features can be included in our pipeline. This is where `FeatureUnion` comes to the rescue! This is a tool that can combine multiple transformer objects into one object whose output is the union of the individual outputs.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to combine the features generated by a `CountVectorizer` + `tfidfTransformer` pipeline with the features generated by our function `special_features()`. To combine these two approaches we need them to take the same input, so we'll use the corpus of unprocessed email strings. Let's start by making a pipeline of `CountVectorizer` + `tfidfTransformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a pipeline for vectorizing bag of words\n",
    "objs = [(\"vect\", CountVectorizer(preprocessor=word_salad)), \n",
    "        (\"tfidf\", TfidfTransformer())]\n",
    "pipe_vectorizer = Pipeline(objs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a Custom Transformer \n",
    "Now we need to turn our custom function `special_features` into a protypical sklearn `transformer` before we can union it with the vectorization pipeline. Luckily there is a helper function for this called `FunctionTransformer`! First let's make a wrapper for `special_featues` so that it can take the same input as the vectorization pipeline: a list of raw email body strings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def special_features(body):\n",
    "    '''Produce some special features from email body.'''\n",
    "    # Parse HTML extract content only (but count tags)\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    nhtml = len(soup.find_all())\n",
    "    nlinks = len(soup.find_all(\"a\"))\n",
    "    body = soup.get_text()\n",
    "    \n",
    "    # count all URLs \n",
    "    nhttps = body.count(\"http\")\n",
    "\n",
    "    # count all email addresses\n",
    "#     regx = re.compile(r\"\\b[^\\s]+@[^\\s]+[.][^\\s]+\\b\")\n",
    "#     nemails = len(regx.findall(string=body))\n",
    "    nemails = body.count(\"@\")\n",
    "    \n",
    "    # Count uppercases \n",
    "    nupper = len([charup for charup, char in zip(body, body.lower()) if charup != char])\n",
    "    \n",
    "    # Count and replace all numbers (integer and float)\n",
    "    regx = re.compile(r\"\\b[\\d.]+\\b\")\n",
    "    nnum = len(regx.findall(string=body))\n",
    "\n",
    "    # Count number of special punctuation\n",
    "    ndollar, nexclaim, nquest = body.count(\"$\"), body.count(\"!\"), body.count(\"?\")\n",
    "\n",
    "    # Count carriage returs and blank lines\n",
    "    nblanks, nnewlines = body.count(\"\\n\\n\"), body.count(\"\\n\")\n",
    "\n",
    "    # Get total word count\n",
    "    freqns = np.array([nemails, nhttps, nexclaim, nquest, ndollar,\n",
    "              nblanks, nnewlines, nhtml, nlinks, nnum])/len(body)\n",
    "#     freqns = [count/nchars for count in counts]\n",
    "\n",
    "    return freqns\n",
    "\n",
    "def special_features_wrapper(bodies):\n",
    "    xs = [special_features(body) for body in bodies]\n",
    "    X = np.array(xs)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since most transformers in a preprocessing pipeline typically operate on a on a matrix of numbers, `FunctionTransformer` thinks it is being helpful by creating a transformer who checks it's input type. Our input is instead a list of raw email strings, so we need to call `FunctionTransformer` with kwarg `validate=False` to turn off this default checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Turn our custom function into a sklearn transformer!\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "special_features_transformer = FunctionTransformer(special_features_wrapper, validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00127959,  0.0006398 ,  0.        ,  0.        ,  0.00191939,\n",
       "         0.01023672,  0.03262956,  0.0006398 ,  0.        ],\n",
       "       [ 0.00110988,  0.00221976,  0.00221976,  0.00110988,  0.        ,\n",
       "         0.00665927,  0.0299667 ,  0.        ,  0.        ],\n",
       "       [ 0.00057045,  0.0011409 ,  0.0011409 ,  0.        ,  0.        ,\n",
       "         0.00399315,  0.02224758,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test out the transform method of our new transformer\n",
    "special_features_transformer.transform(emails_raw[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's a little bonus for you, instead of using sklearn's helper function, you can make a custom transformer by defining your own class who inherits from sklearn's base classes. Your class should define a `transform` method that executes your custom preprocessing / feature extraction functionality. Refer to[this helpful (and pretty) blog post](https://michelleful.github.io/code-blog/2015/06/20/pipelines/) for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class CustomTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return special_features_wrapper(X)  # your custom feature extraction / preprocessing\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # generally does nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  1,  0,  0,  3, 16, 51,  1,  0],\n",
       "       [ 1,  2,  2,  1,  0,  6, 27,  0,  0],\n",
       "       [ 1,  2,  2,  0,  0,  7, 39,  0,  0]])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate our custom transformer class and try it out\n",
    "mytransformer = CustomTransformer()\n",
    "mytransformer.transform(emails_raw[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelining Special Feature Extraction With Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to make sure that our special features are all properly scaled, to facilitate the numerical optimization of the SVM in fitting. That means we need to create a pipeline which is our special feature transformer chained with sklearn's `StandardScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a pipeline for standardizing our special features\n",
    "objs = [(\"specials\", special_features_transformer),\n",
    "       (\"scaler\", StandardScaler())]\n",
    "pipe_specialfeats = Pipeline(objs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Union of Two Pipelines\n",
    "Now we are ready for our feature union! We will create an object whose output is our full feature vector, which will be the union of a tfidf-transformed word occurrence vector with a scaled special feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a feature union of vectorizer pipeline and custom transformer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "objs = [(\"vectscaled\", pipe_vectorizer), \n",
    "        (\"specialfeats\", pipe_specialfeats)]\n",
    "union = FeatureUnion(objs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are you confused yet? We made a pipeline from `CountVectorizer` and `tfidfTranformer` to get an object that takes a raw email body and spits out an occurrence vector. Then we used `FunctionTransformer` to turn our custom feature extraction function into a transformer that takes in a raw email body and spits a vector of other features. Then we created a union of these two things, which is itself a transformer object that will take in a raw email body and spitout the union of the ocurrence vector with the special features vector. \n",
    "\n",
    "## Final Complete Pipeline with SVM Estimator\n",
    "Now we need a *final* pipeline that combines our feature union object with the actual SVM estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a pipeline of feature union with svm estimator\n",
    "objs = [(\"union\", union),\n",
    "       (\"svm\", SVC(kernel=\"rbf\"))]\n",
    "pipe_full = Pipeline(objs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search With the Full Pipeline\n",
    "Now we can do a grid search with the full pipeline and using the raw emails as our input. Note that to refer to parameters of objects that are nested within pipelines you use the `__` syntax to go down as many layers as needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('union', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('vectscaled', Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None...,\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))]),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'union__vectscaled__vect__min_df': (0.01,), 'svm__gamma': (0.1,), 'svm__C': (1,), 'union__vectscaled__vect__ngram_range': ((1, 1),), 'union__vectscaled__tfidf__use_idf': (False,)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify parameters of the pipeline and their ranges for grid search\n",
    "# params = {\n",
    "#     'union__vectscaled__vect__min_df': (0.01, 0.03, 0.06),\n",
    "#     'union__vectscaled__vect__ngram_range': ((1, 1),(1, 2)),  # unigrams or bigrams\n",
    "#     'union__vectscaled__tfidf__use_idf': (True,),\n",
    "#     'svm__C': np.logspace(-2, 2, 10),\n",
    "#     'svm__gamma': np.logspace(-3, 1, 10),\n",
    "# }\n",
    "\n",
    "params = {\n",
    "    'union__vectscaled__vect__min_df': (0.01,),\n",
    "    'union__vectscaled__vect__ngram_range': ((1, 1),),  # unigrams or bigrams\n",
    "    'union__vectscaled__tfidf__use_idf': (False,),\n",
    "    'svm__C': (1,),\n",
    "    'svm__gamma': (0.1,)\n",
    "}\n",
    "\n",
    "\n",
    "# Construct and fit our grid search object\n",
    "search = GridSearchCV(pipe_full, param_grid=params)\n",
    "search.fit(tempX, tempy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'svm__C': (1,),\n",
       " 'svm__gamma': (0.1,),\n",
       " 'union__vectscaled__tfidf__use_idf': (False,),\n",
       " 'union__vectscaled__vect__min_df': (0.01,),\n",
       " 'union__vectscaled__vect__ngram_range': ((1, 1),)}"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.param_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having specified a maximum n-gram size of two, we should now see single words and word pairs in our fitted vocab list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_svm__C</th>\n",
       "      <th>param_svm__gamma</th>\n",
       "      <th>param_union__vectscaled__tfidf__use_idf</th>\n",
       "      <th>param_union__vectscaled__vect__min_df</th>\n",
       "      <th>param_union__vectscaled__vect__ngram_range</th>\n",
       "      <th>params</th>\n",
       "      <th>...</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.840653</td>\n",
       "      <td>0.932307</td>\n",
       "      <td>0.724138</td>\n",
       "      <td>0.965789</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{'union__vectscaled__vect__min_df': 0.01, 'uni...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.102011</td>\n",
       "      <td>0.24252</td>\n",
       "      <td>0.056661</td>\n",
       "      <td>0.024214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0       1.840653         0.932307         0.724138          0.965789   \n",
       "\n",
       "  param_svm__C param_svm__gamma param_union__vectscaled__tfidf__use_idf  \\\n",
       "0            1              0.1                                   False   \n",
       "\n",
       "  param_union__vectscaled__vect__min_df  \\\n",
       "0                                  0.01   \n",
       "\n",
       "  param_union__vectscaled__vect__ngram_range  \\\n",
       "0                                     (1, 1)   \n",
       "\n",
       "                                              params       ...         \\\n",
       "0  {'union__vectscaled__vect__min_df': 0.01, 'uni...       ...          \n",
       "\n",
       "   split0_test_score  split0_train_score  split1_test_score  \\\n",
       "0                0.7                 1.0                0.8   \n",
       "\n",
       "   split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
       "0            0.947368           0.666667                0.95      0.102011   \n",
       "\n",
       "   std_score_time  std_test_score  std_train_score  \n",
       "0         0.24252        0.056661         0.024214  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(search.cv_results_)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tempX = emails_raw[0:20] + emails_raw[-10:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tempy = y[0:20] + y[-10:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
